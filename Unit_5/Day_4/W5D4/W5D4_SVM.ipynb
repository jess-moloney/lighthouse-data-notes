{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Support Vector Machines\n",
    "\n",
    "Original notebook by: Socorro Dominguez  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install git+https://github.com/mgelbart/plot-classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from plot_classifier import plot_classifier\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_df = pd.read_csv('canada_usa_cities.csv')\n",
    "train_df, test_df = train_test_split(cities_df, test_size=0.2, random_state=123)\n",
    "canada = train_df.query('country == \"Canada\"')\n",
    "usa = train_df.query('country == \"USA\"')\n",
    "plt.scatter(canada[\"longitude\"], canada[\"latitude\"], color=\"red\", alpha=0.6)\n",
    "plt.scatter(usa[\"longitude\"], usa[\"latitude\"], color=\"blue\", alpha=0.6)\n",
    "plt.ylabel(\"latitude\")\n",
    "plt.xlabel(\"longitude\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into training/validation and testing set\n",
    "X_train, y_train = train_df.drop(columns=['country']).values, train_df['country'].values\n",
    "X_test, y_test = test_df.drop(columns=['country']).values, test_df['country'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svm = SVC(gamma=0.01, C=1.0) # Ignore gamma for now\n",
    "svm.fit(X_train, y_train)\n",
    "print(\"Train scores: \", round(svm.score(X_train, y_train), 4))\n",
    "print(\"Test scores: \", round(svm.score(X_test, y_test), 4))\n",
    "\n",
    "# You can think of SVM with RBF kernel as \"smooth KNN\"\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.title(\"SVC\")\n",
    "plot_classifier(X_train, y_train, svm, ax=plt.gca());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support vectors \n",
    "\n",
    " - Each training example either is or isn't a \"support vector\"\n",
    "   - This gets decided during `fit`\n",
    "\n",
    " - **Main insight: the decision boundary only depends on the support vectors.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "SVM is going to leverage the dataset and find the best values **$w^{(*)}$** and **$b^{(*)}$** for parameters $w$ and $b$. \n",
    "\n",
    "Then, the learning algorithm is defined as:\n",
    "\n",
    ">$f(x) = sign(w^{(*)}x + b^{(*)})$\n",
    "\n",
    "Now, to predict whether a **new** data point is positive or negative using an SVM model, you multiply its feature vector by **$w^{(*)}$**, add **$b^{(*)}$** and take the sign of the result. \n",
    "\n",
    "If you get +1, then, the data point belongs to the positive class. \n",
    "If you get a -1, then the data point belongs to the negative class. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We want a hyperplane that separates positive examples from negative ones with the largest margin. \n",
    "\n",
    "If the margin or decision boundary is large, it contributes to a better generalization, that is how well the model will classify new examples in the future. \n",
    "\n",
    "For two-dimensional feature vectors, we can easily visualize the solution. The blue and orange circles represent, respectively, positive and negative examples. The line given by $wx + b = 0$ is the decision boundary.\n",
    "\n",
    "That’s how Support Vector Machines work. This particular version of the algorithm builds the so-called linear model. It’s called linear because the decision boundary is a straight line (or a plane, or a hyperplane)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# demo with a synthetic data set\n",
    "n = 20\n",
    "X = np.random.randn(n,2)\n",
    "y = np.random.choice((-1,+1),size=n)\n",
    "X[y>0,0] -= 2\n",
    "X[y>0,1] += 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "svm = SVC(kernel=\"linear\", C=1e6) # ignore the C=1e6 for now\n",
    "svm.fit(X,y)\n",
    "\n",
    "plt.figure(figsize=(4,4))\n",
    "\n",
    "plot_classifier(X, y, svm, ax=plt.gca())\n",
    "plt.scatter(*svm.support_vectors_.T, marker=\"o\", edgecolor=\"yellow\", facecolor=\"none\", s=120);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The support vectors (SVs) are shown in yellow.\n",
    "- These are the examples that \"support\" the boundary. \n",
    "\n",
    "Below: let's try removing all other examples, keeping only the SVs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "sv = svm.support_\n",
    "not_sv = list(set(range(n)) - set(sv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# remove all non-support vectors\n",
    "X3 = np.delete(X,not_sv,0)\n",
    "y3 = np.delete(y,not_sv,0)\n",
    "\n",
    "svm3 = SVC(kernel=\"linear\", C=1e6)\n",
    "svm3.fit(X3,y3)\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.subplot(1,2,1)\n",
    "plot_classifier(X,y,svm, ax=plt.gca())\n",
    "plt.scatter(*svm.support_vectors_.T, marker=\"o\", edgecolor=\"yellow\", facecolor=\"none\", s=120);\n",
    "plt.title(\"Original\");\n",
    "plt.subplot(1,2,2)\n",
    "plot_classifier(X3,y3,svm3, ax=plt.gca(), lims=(X[:,0].min()-1,X[:,0].max()+1,X[:,1].min()-1,X[:,1].max()+1))\n",
    "plt.title(\"SVs only\");\n",
    "\n",
    "print(svm.coef_)\n",
    "print(svm3.coef_)\n",
    "print(\"The coefficients are the same!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# remove a support vector\n",
    "X2 = np.delete(X,sv[1],0)\n",
    "y2 = np.delete(y,sv[1],0)\n",
    "\n",
    "svm2 = SVC(kernel=\"linear\", C=1e6)\n",
    "svm2.fit(X2,y2);\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.subplot(1,2,1)\n",
    "plot_classifier(X,y,svm, ax=plt.gca())\n",
    "plt.scatter(*svm.support_vectors_.T, marker=\"o\", edgecolor=\"yellow\", facecolor=\"none\", s=120);\n",
    "plt.title(\"Original\");\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plot_classifier(X2,y2,svm2, ax=plt.gca())\n",
    "# plt.scatter(*svm2.support_vectors_.T, marker=\"o\", edgecolor=\"yellow\", facecolor=\"none\", s=120);\n",
    "plt.scatter(svm.support_vectors_[0,0], svm.support_vectors_[0,1], marker=\"x\", c=\"yellow\")\n",
    "plt.title(\"With one SV removed\");\n",
    "\n",
    "print(svm.coef_)\n",
    "print(svm2.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- The **margin** is the distance from the boundary to the nearest point(s).\n",
    "- Support vector machines try to maximize the margin while minimizing misclassification.\n",
    "- Intuitively, more margin is good because it leaves more \"room\" before we make an error.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plot_classifier(X, y, svm, ax=plt.gca());\n",
    "plt.scatter(*svm.support_vectors_.T, marker=\"o\", edgecolor=\"yellow\", facecolor=\"none\", s=120);\n",
    "plt.axis('equal');\n",
    "plt.axis('square');\n",
    "\n",
    "def SV_proj(svm):\n",
    "    v = svm.support_vectors_\n",
    "    s = np.array([svm.coef_.flatten()[1], -svm.coef_.flatten()[0]])\n",
    "    w = svm.coef_\n",
    "    return (v@s[:,None])/(s@s) * s - w/(w@w.T)*svm.intercept_\n",
    "proj = SV_proj(svm)\n",
    "\n",
    "for i in range(len(proj)):\n",
    "    p = proj[i]\n",
    "    sv = svm.support_vectors_[i]\n",
    "    plt.plot((p[0],sv[0]),(p[1],sv[1]), 'yellow')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters of SVM \n",
    "\n",
    "- Key hyperparameters of `rbf` SVM are\n",
    "    - `gamma`\n",
    "    - `C`\n",
    "    \n",
    "- We are not equipped to understand the meaning of these parameters at this point but you are expected to describe their relation to the fundamental tradeoff. \n",
    "\n",
    "See [`scikit-learn`'s explanation of RBF SVM parameters](https://scikit-learn.org/stable/auto_examples/svm/plot_rbf_parameters.html). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relation of `gamma` and the fundamental trade-off\n",
    "\n",
    "- `gamma` controls the complexity (fundamental trade-off), just like other hyperparameters we've seen.\n",
    "  - larger `gamma` $\\rightarrow$ more complex\n",
    "  - smaller `gamma` $\\rightarrow$ less complex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relation of `C` and the fundamental trade-off\n",
    "\n",
    "- `C` _also_ affects the fundamental tradeoff\n",
    "    - larger `C` $\\rightarrow$ more complex \n",
    "    - smaller `C` $\\rightarrow$ less complex "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Dealing with Inherent Non-Linearity\n",
    "SVM can be adapted to work with datasets that cannot be separated by a hyperplane in its original space. Indeed, if we manage to transform the original space into a space of higher dimensionality, we could hope that the examples will become linearly separable in this transformed space. The image below shows this process in action: the image on the left is the data in the original 2 dimensions and the one on the right is a projection into 3 dimensions. In this example, in 3 dimensions, it is easy for the algorithm to find the plane that separates the two classes. The circle in the right image is the 3 dimensional decision boundary projected back into the original 2 dimensions, which is why it is a circle. \n",
    "\n",
    "Add extra dimension | Projection back to 2D\n",
    "- | - \n",
    "![image.png](img/SVM_kernel.png) | ![image.png](img/SVM_circle.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "By using the **kernel trick**, we can get rid of a costly transformation of original feature vectors into higher dimensional vectors and avoid computing their dot-product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "n = 20\n",
    "d = 2\n",
    "np.random.seed(0)\n",
    "X = np.random.randn(n,d)\n",
    "y = np.sum(X**2,axis=1) < 0.4\n",
    "\n",
    "plt.scatter(X[:,0], X[:,1], c=y);\n",
    "plt.xlabel(\"$x_{1}$\", fontsize=20);\n",
    "plt.ylabel(\"$x_{2}$\", fontsize=20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "svm = SVC(kernel=\"linear\")\n",
    "svm.fit(X,y)\n",
    "\n",
    "plot_classifier(X,y,svm)\n",
    "\n",
    "print(\"Training accuracy\", svm.score(X,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "Z = X**2\n",
    "\n",
    "plt.scatter(Z[:,0], Z[:,1], c=y);\n",
    "plt.xlabel(\"$z_{i1}$\", fontsize=20);\n",
    "plt.ylabel(\"$z_{i2}$\", fontsize=20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "svm = SVC(kernel=\"linear\", C=100)\n",
    "svm.fit(Z,y)\n",
    "\n",
    "plot_classifier(Z,y,svm)\n",
    "\n",
    "print(\"Training accuracy\", svm.score(Z,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plot_classifier(X, y, svm, transformation=lambda X: X**2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "svm = SVC(kernel=\"poly\", degree=2)\n",
    "svm.fit(X,y)\n",
    "\n",
    "plot_classifier(X,y,svm)\n",
    "\n",
    "print(\"Training accuracy\", svm.score(X,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "svm = SVC(kernel=\"rbf\")\n",
    "svm.fit(X,y)\n",
    "\n",
    "plot_classifier(X,y,svm)\n",
    "\n",
    "print(\"Training accuracy\", svm.score(X,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
