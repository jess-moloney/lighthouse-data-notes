{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent\n",
    "\n",
    "In this notebook we will implement gradient descent for a single neuron. To simplify this as much as possible we will assume we have no bias `b = 0` and no activation function. The image below shows the neuron we will be coding and the relevant equations needed to update the weight using gradient descent. \n",
    "\n",
    "<img src=\"Gradient Descent Notebook-1.jpg\" width=600 align=\"center\">\n",
    "\n",
    "The recipe we are following is:\n",
    "1. make a prediction for our single sample\n",
    "2. calculate the loss for this single sample\n",
    "3. calculate the value of the derivative for this single sample\n",
    "4. use the derivative and the learning rate to update the value of the weight\n",
    "5. repeat until the loss is a minimum (that is, stops decreasing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stochastic Gradient Descent: A single sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 0 weight: 0.2930  prediction: 0.2000  Loss: 14.44000000\n",
      "iteration: 10 weight: 1.4153  prediction: 2.6984  Loss: 1.69418978\n",
      "iteration: 20 weight: 1.7997  prediction: 3.5542  Loss: 0.19877279\n",
      "iteration: 30 weight: 1.9314  prediction: 3.8473  Loss: 0.02332125\n",
      "iteration: 40 weight: 1.9765  prediction: 3.9477  Loss: 0.00273619\n",
      "iteration: 50 weight: 1.9920  prediction: 3.9821  Loss: 0.00032103\n",
      "iteration: 60 weight: 1.9972  prediction: 3.9939  Loss: 0.00003766\n",
      "iteration: 70 weight: 1.9991  prediction: 3.9979  Loss: 0.00000442\n",
      "iteration: 80 weight: 1.9997  prediction: 3.9993  Loss: 0.00000052\n",
      "iteration: 90 weight: 1.9999  prediction: 3.9998  Loss: 0.00000006\n",
      "iteration: 100 weight: 2.0000  prediction: 3.9999  Loss: 0.00000001\n",
      "iteration: 110 weight: 2.0000  prediction: 4.0000  Loss: 0.00000000\n"
     ]
    }
   ],
   "source": [
    "y = 4 # correct answer\n",
    "x = 2 # we have one sample and one feature \n",
    "w = 0.1 # initial value for our weight\n",
    "\n",
    "lr = 0.0127 # the learning rate\n",
    "\n",
    "n_iterations = 120 # number of iterations of gradient descent, that is, number of times we update w\n",
    "\n",
    "for i in range(n_iterations):\n",
    "    y_hat = w * x # make prediction using current value for w\n",
    "    L = (y - y_hat)**2  # calculate the loss (our loss function = mean squared error)\n",
    "    dL_dw = -2 * (y - y_hat)*x   # calculate derivative needed to update the weight (see image above) \n",
    "    w = w - lr*dL_dw # update the weight\n",
    "    # the code below allows you to print out answers after every x iterations; adjust to suit your needs\n",
    "    if (i%10 == 0):\n",
    "        print(f\"iteration: {i} weight: {w:.4f}  prediction: {y_hat:.4f}  Loss: {L:.8f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stochastic Gradient Descent: Multiple samples\n",
    "\n",
    "We will now repeat what we did above but now our training data will have 5 samples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [1, 2, 3, 3, 4]  # 5 samples with 1 feature value each\n",
    "Y = [2, 1, 4, 2, 5] # correct answers for each sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the data so we can see what we are dealing with. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x2d9ffddf5e0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiMAAAGdCAYAAADAAnMpAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy89olMNAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAlG0lEQVR4nO3df3DU9Z3H8deGH7u2ZrekZ7IBFo4WGggxYrCUjV5BBQEZhtwfd5pBg3fgnEyYSdqr1Th2UJnOQq21zOBFqqO5K2Uy0l5wSjE0wiUZTFB+JHMJVqbYSGLNJp1TdpPUbJnke38wrKxkw+7mxyc/no+Z7x/72c8n3/d+5st+X3z3s9+1WZZlCQAAwJAk0wUAAIDJjTACAACMIowAAACjCCMAAMAowggAADCKMAIAAIwijAAAAKMIIwAAwKippguIRX9/vz755BMlJyfLZrOZLgcAAMTAsix1dXVp5syZSkqKfv1jXISRTz75RB6Px3QZAAAgAW1tbZo9e3bU58dFGElOTpZ05cU4nU7D1QAAgFgEg0F5PJ7weTyacRFGrn4043Q6CSMAAIwzN1piwQJWAABgFGEEAAAYRRgBAABGEUYAAIBRhBEAAGAUYQQAABhFGAEAAEYRRgAAgFHj4qZnAABg+PX1W3qv5VN1dvUqNdmhZfNSNCVp9H8DLq4rI88884xsNlvEtnDhwkHHHDx4UAsXLpTD4dCtt96qI0eODKlgAAAwdJXN7bpr93Hlv3JSReWNyn/lpO7afVyVze2jXkvcH9MsXrxY7e3t4e3EiRNR+9bV1Sk/P19btmxRQ0OD8vLylJeXp+bm5iEVDQAAElfZ3K5t+8+qPdAb0e4P9Grb/rOjHkjiDiNTp06V2+0Ob3/3d38Xte+ePXu0du1aPf7441q0aJF27typnJwc7d27d0hFAwCAxPT1W3r2t+/LGuC5q23P/vZ99fUP1GNkxB1G/vjHP2rmzJn6xje+oU2bNqm1tTVq3/r6eq1atSqibc2aNaqvrx90H6FQSMFgMGIDAABD917Lp9ddEbmWJak90Kv3Wj4dtZriCiPf+c53VFZWpsrKSpWWlqqlpUX/8A//oK6urgH7+/1+paWlRbSlpaXJ7/cPuh+fzyeXyxXePB5PPGUCAIAoOruiB5FE+g2HuMLIunXr9E//9E/Kzs7WmjVrdOTIEV26dElvvPHGsBZVUlKiQCAQ3tra2ob17wMAMFmlJjuGtd9wGNJXe7/2ta/pW9/6li5cuDDg8263Wx0dHRFtHR0dcrvdg/5du90uu90+lNIAAMAAls1LUbrLIX+gd8B1IzZJbteVr/mOliHd9Ky7u1sffvih0tPTB3ze6/Xq2LFjEW1VVVXyer1D2S0AAEjQlCSbdmzIlHQleFzr6uMdGzJH9X4jcYWRH/zgB6qpqdFHH32kuro6/eM//qOmTJmi/Px8SVJBQYFKSkrC/YuKilRZWakXXnhBH3zwgZ555hmdPn1a27dvH95XAQAAYrY2K12lD+XI7Yr8KMbtcqj0oRytzRr4IsNIietjmo8//lj5+fn6v//7P91yyy266667dPLkSd1yyy2SpNbWViUlfZFvcnNzdeDAAT399NN66qmntGDBAh06dEhZWVnD+yoAAEBc1mala3Wme0zcgdVmWdbofZE4QcFgUC6XS4FAQE6n03Q5AAAgBrGev/mhPAAAYBRhBAAAGEUYAQAARhFGAACAUYQRAABgFGEEAAAYRRgBAABGEUYAAIBRhBEAAGAUYQQAABhFGAEAAEYRRgAAgFGEEQAAYBRhBAAAGEUYAQAARhFGAACAUYQRAABgFGEEAAAYRRgBAABGEUYAAIBRhBEAAGAUYQQAABhFGAEAAEYRRgAAgFGEEQAAYBRhBAAAGEUYAQAARhFGAACAUYQRAABgFGEEAAAYRRgBAABGDSmM7Nq1SzabTcXFxVH7lJWVyWazRWwOh2MouwUAABPI1EQHnjp1Svv27VN2dvYN+zqdTp0/fz782GazJbpbAAAwwSR0ZaS7u1ubNm3SK6+8ohkzZtywv81mk9vtDm9paWmJ7BYAAExACYWRwsJCrV+/XqtWrYqpf3d3t+bOnSuPx6ONGzfq3Llzg/YPhUIKBoMRGwAAmJjiDiPl5eU6e/asfD5fTP0zMjL02muv6c0339T+/fvV39+v3Nxcffzxx1HH+Hw+uVyu8ObxeOItEwAAjBM2y7KsWDu3tbXpjjvuUFVVVXityMqVK7VkyRL9/Oc/j+lvXL58WYsWLVJ+fr527tw5YJ9QKKRQKBR+HAwG5fF4FAgE5HQ6Yy0XAAAYFAwG5XK5bnj+jmsB65kzZ9TZ2amcnJxwW19fn2pra7V3716FQiFNmTJl0L8xbdo03X777bpw4ULUPna7XXa7PZ7SAADAOBVXGLn33nvV1NQU0fYv//IvWrhwoZ544okbBhHpSnhpamrS/fffH1+lAABgQoorjCQnJysrKyui7atf/aq+/vWvh9sLCgo0a9as8JqS5557TsuXL9f8+fN16dIlPf/887p48aK2bt06TC8BAACMZwnfZySa1tZWJSV9sS72s88+06OPPiq/368ZM2Zo6dKlqqurU2Zm5nDvGgAAjENxLWA1JdYFMAAAYOyI9fzNb9MAAACjCCMAAMAowggAADCKMAIAAIwijAAAAKMIIwAAwCjCCAAAMIowAgAAjCKMAAAAowgjAADAKMIIAAAwijACAACMIowAAACjCCMAAMAowggAADCKMAIAAIwijAAAAKMIIwAAwCjCCAAAMIowAgAAjCKMAAAAowgjAADAKMIIAAAwijACAACMIowAAACjCCMAAMAowggAADCKMAIAAIwijAAAAKMIIwAAwKippgsAANxYX7+l91o+VWdXr1KTHVo2L0VTkmymywKGxZCujOzatUs2m03FxcWD9jt48KAWLlwoh8OhW2+9VUeOHBnKbgFgUqlsbtddu48r/5WTKipvVP4rJ3XX7uOqbG43XRowLBIOI6dOndK+ffuUnZ09aL+6ujrl5+dry5YtamhoUF5envLy8tTc3JzorgFg0qhsbte2/WfVHuiNaPcHerVt/1kCCSaEhMJId3e3Nm3apFdeeUUzZswYtO+ePXu0du1aPf7441q0aJF27typnJwc7d27N6GCAWCy6Ou39Oxv35c1wHNX25797fvq6x+oBzB+JBRGCgsLtX79eq1ateqGfevr66/rt2bNGtXX10cdEwqFFAwGIzYAmGzea/n0uisi17IktQd69V7Lp6NXFDAC4l7AWl5errNnz+rUqVMx9ff7/UpLS4toS0tLk9/vjzrG5/Pp2Wefjbc0AJhQOruiB5FE+gFjVVxXRtra2lRUVKRf/epXcjgcI1WTSkpKFAgEwltbW9uI7QsAxqrU5NjeZ2PtB4xVcV0ZOXPmjDo7O5WTkxNu6+vrU21trfbu3atQKKQpU6ZEjHG73ero6Iho6+jokNvtjrofu90uu90eT2kAMOEsm5eidJdD/kDvgOtGbJLcritf8wXGs7iujNx7771qampSY2NjeLvjjju0adMmNTY2XhdEJMnr9erYsWMRbVVVVfJ6vUOrHAAmuClJNu3YkCnpSvC41tXHOzZkcr8RjHtxXRlJTk5WVlZWRNtXv/pVff3rXw+3FxQUaNasWfL5fJKkoqIirVixQi+88ILWr1+v8vJynT59Wr/4xS+G6SUAwMS1NitdpQ/l6Nnfvh+xmNXtcmjHhkytzUo3WB0wPIb9Dqytra1KSvrigktubq4OHDigp59+Wk899ZQWLFigQ4cOXRdqAAADW5uVrtWZbu7AignLZlnWmP+CejAYlMvlUiAQkNPpNF0OAACIQaznb34oDwAAGEUYAQAARhFGAACAUYQRAABgFGEEAAAYRRgBAABGEUYAAIBRhBEAAGAUYQQAABhFGAEAAEYRRgAAgFGEEQAAYBRhBAAAGEUYAQAARhFGAACAUYQRAABgFGEEAAAYRRgBAABGEUYAAIBRhBEAAGAUYQQAABhFGAEAAEYRRgAAgFGEEQAAYBRhBAAAGEUYAQAARhFGAACAUYQRAABgFGEEAAAYRRgBAABGEUYAAIBRcYWR0tJSZWdny+l0yul0yuv16q233orav6ysTDabLWJzOBxDLhoAAEwcU+PpPHv2bO3atUsLFiyQZVn6z//8T23cuFENDQ1avHjxgGOcTqfOnz8ffmyz2YZWMQAAmFDiCiMbNmyIePzjH/9YpaWlOnnyZNQwYrPZ5Ha7E68QAABMaAmvGenr61N5ebl6enrk9Xqj9uvu7tbcuXPl8Xi0ceNGnTt37oZ/OxQKKRgMRmwAAGBiijuMNDU16eabb5bdbtdjjz2miooKZWZmDtg3IyNDr732mt58803t379f/f39ys3N1ccffzzoPnw+n1wuV3jzeDzxlgkAAMYJm2VZVjwD/va3v6m1tVWBQEC//vWv9eqrr6qmpiZqILnW5cuXtWjRIuXn52vnzp1R+4VCIYVCofDjYDAoj8ejQCAgp9MZT7kAAMCQYDAol8t1w/N3XGtGJGn69OmaP3++JGnp0qU6deqU9uzZo3379t1w7LRp03T77bfrwoULg/az2+2y2+3xlgYAAMahId9npL+/P+IqxmD6+vrU1NSk9PT0oe4WAABMEHFdGSkpKdG6des0Z84cdXV16cCBA6qurtbRo0clSQUFBZo1a5Z8Pp8k6bnnntPy5cs1f/58Xbp0Sc8//7wuXryorVu3Dv8rAQAA41JcYaSzs1MFBQVqb2+Xy+VSdna2jh49qtWrV0uSWltblZT0xcWWzz77TI8++qj8fr9mzJihpUuXqq6uLqb1JQAAYHKIewGrCbEugAEAAGNHrOdvfpsGAAAYRRgBAABGEUYAAIBRhBEAAGAUYQQAABhFGAEAAEYRRgAAgFGEEQAAYBRhBAAAGEUYAQAARhFGAACAUYQRAABgFGEEAAAYRRgBAABGEUYAAIBRhBEAAGAUYQQAABhFGAEAAEYRRgAAgFGEEQAAYBRhBAAAGEUYAQAARhFGAACAUYQRAABgFGEEAAAYRRgBAABGEUYAAIBRhBEAAGAUYQQAABhFGAEAAEYRRgAAgFFxhZHS0lJlZ2fL6XTK6XTK6/XqrbfeGnTMwYMHtXDhQjkcDt166606cuTIkAoGAAATS1xhZPbs2dq1a5fOnDmj06dP65577tHGjRt17ty5AfvX1dUpPz9fW7ZsUUNDg/Ly8pSXl6fm5uZhKR4AAIx/NsuyrKH8gZSUFD3//PPasmXLdc898MAD6unp0eHDh8Nty5cv15IlS/Tyyy/HvI9gMCiXy6VAICCn0zmUcgEAwCiJ9fyd8JqRvr4+lZeXq6enR16vd8A+9fX1WrVqVUTbmjVrVF9fP+jfDoVCCgaDERsAAJiY4g4jTU1Nuvnmm2W32/XYY4+poqJCmZmZA/b1+/1KS0uLaEtLS5Pf7x90Hz6fTy6XK7x5PJ54ywQAAONE3GEkIyNDjY2Nevfdd7Vt2zZt3rxZ77///rAWVVJSokAgEN7a2tqG9e8DAICxY2q8A6ZPn6758+dLkpYuXapTp05pz5492rdv33V93W63Ojo6Ito6OjrkdrsH3Yfdbpfdbo+3NAAAMA4N+T4j/f39CoVCAz7n9Xp17NixiLaqqqqoa0wAAMDkE9eVkZKSEq1bt05z5sxRV1eXDhw4oOrqah09elSSVFBQoFmzZsnn80mSioqKtGLFCr3wwgtav369ysvLdfr0af3iF78Y/lcCAADGpbjCSGdnpwoKCtTe3i6Xy6Xs7GwdPXpUq1evliS1trYqKemLiy25ubk6cOCAnn76aT311FNasGCBDh06pKysrOF9FQAAYNwa8n1GRgP3GQEAYPwZ8fuMAAAADAfCCAAAMIowAgAAjCKMAAAAowgjAADAKMIIAAAwijACAACMIowAAACjCCMAAMAowggAADCKMAIAAIwijAAAAKMIIwAAwCjCCAAAMIowAgAAjCKMAAAAowgjAADAKMIIAAAwijACAACMIowAAACjCCMAAMAowggAADCKMAIAAIwijAAAAKMIIwAAwCjCCAAAMIowAgAAjCKMAAAAowgjAADAKMIIAAAwijACAACMiiuM+Hw+ffvb31ZycrJSU1OVl5en8+fPDzqmrKxMNpstYnM4HEMqGgAATBxxhZGamhoVFhbq5MmTqqqq0uXLl3Xfffepp6dn0HFOp1Pt7e3h7eLFi0MqGgAATBxT4+lcWVkZ8bisrEypqak6c+aMvvvd70YdZ7PZ5Ha7E6sQAABMaENaMxIIBCRJKSkpg/br7u7W3Llz5fF4tHHjRp07d27Q/qFQSMFgMGIDAAATU8JhpL+/X8XFxbrzzjuVlZUVtV9GRoZee+01vfnmm9q/f7/6+/uVm5urjz/+OOoYn88nl8sV3jweT6JlAgCAMc5mWZaVyMBt27bprbfe0okTJzR79uyYx12+fFmLFi1Sfn6+du7cOWCfUCikUCgUfhwMBuXxeBQIBOR0OhMpFwAAjLJgMCiXy3XD83dca0au2r59uw4fPqza2tq4gogkTZs2TbfffrsuXLgQtY/dbpfdbk+kNAAAMM7E9TGNZVnavn27KioqdPz4cc2bNy/uHfb19ampqUnp6elxjwUAABNPXFdGCgsLdeDAAb355ptKTk6W3++XJLlcLt10002SpIKCAs2aNUs+n0+S9Nxzz2n58uWaP3++Ll26pOeff14XL17U1q1bh/mlAACA8SiuMFJaWipJWrlyZUT766+/rkceeUSS1NraqqSkLy64fPbZZ3r00Ufl9/s1Y8YMLV26VHV1dcrMzBxa5QAAYEJIeAHraIp1AQwAABg7Yj1/89s0AADAKMIIAAAwijACAACMIowAAACjCCMAAMAowggAADCKMAIAAIwijAAAAKMIIwAAwCjCCAAAMIowAgAAjCKMAAAAowgjAADAKMIIAAAwijACAACMIowAAACjCCMAAMAowggAADCKMAIAAIwijAAAAKMIIwAAwCjCCAAAMIowAgAAjCKMAAAAowgjAADAKMIIAAAwijACAACMIowAAACjCCMAAMAowggAADBqqukCTOnrt/Rey6fq7OpVarJDy+alaEqSzXRZADAg3rMwkcUVRnw+n/77v/9bH3zwgW666Sbl5uZq9+7dysjIGHTcwYMH9aMf/UgfffSRFixYoN27d+v+++8fUuFDUdncrmd/+77aA73htnSXQzs2ZGptVrqxugBgILxnYaKL62OampoaFRYW6uTJk6qqqtLly5d13333qaenJ+qYuro65efna8uWLWpoaFBeXp7y8vLU3Nw85OITUdncrm37z0b8o5Ykf6BX2/afVWVzu5G6AGAgvGdhMrBZlmUlOvgvf/mLUlNTVVNTo+9+97sD9nnggQfU09Ojw4cPh9uWL1+uJUuW6OWXX45pP8FgUC6XS4FAQE6nM9Fy1ddv6a7dx6/7R32VTZLb5dCJJ+7h8icA43jPwngX6/l7SAtYA4GAJCklJSVqn/r6eq1atSqibc2aNaqvr486JhQKKRgMRmzD4b2WT6P+o5YkS1J7oFfvtXw6LPsDgKHgPQuTRcJhpL+/X8XFxbrzzjuVlZUVtZ/f71daWlpEW1pamvx+f9QxPp9PLpcrvHk8nkTLjNDZFf0fdSL9AGAk8Z6FySLhMFJYWKjm5maVl5cPZz2SpJKSEgUCgfDW1tY2LH83NdkxrP0AYCTxnoXJIqGv9m7fvl2HDx9WbW2tZs+ePWhft9utjo6OiLaOjg653e6oY+x2u+x2eyKlDWrZvBSluxzyB3o10EKZq5+/LpsX/WMnABgtvGdhsojryohlWdq+fbsqKip0/PhxzZs374ZjvF6vjh07FtFWVVUlr9cbX6XDYEqSTTs2ZEq68o/4Wlcf79iQyUIwAGMC71mYLOIKI4WFhdq/f78OHDig5ORk+f1++f1+ff755+E+BQUFKikpCT8uKipSZWWlXnjhBX3wwQd65plndPr0aW3fvn34XkUc1malq/ShHLldkZc13S6HSh/K4Tv7AMYU3rMwGcT11V6bbeD0/frrr+uRRx6RJK1cuVJ///d/r7KysvDzBw8e1NNPPx2+6dlPfvKTuG56Nlxf7b0WdzMEMJ7wnoXxKNbz95DuMzJaRiKMAACAkTUq9xkBAAAYKsIIAAAwijACAACMIowAAACjCCMAAMAowggAADCKMAIAAIwijAAAAKMIIwAAwCjCCAAAMIowAgAAjCKMAAAAowgjAADAKMIIAAAwijACAACMIowAAACjCCMAAMAowggAADCKMAIAAIwijAAAAKMIIwAAwCjCCAAAMIowAgAAjCKMAAAAowgjAADAKMIIAAAwijACAACMIowAAACjCCMAAMAowggAADCKMAIAAIyKO4zU1tZqw4YNmjlzpmw2mw4dOjRo/+rqatlstus2v9+faM0AAGACiTuM9PT06LbbbtNLL70U17jz58+rvb09vKWmpsa7awAAMAFNjXfAunXrtG7durh3lJqaqq997WtxjwMAABPbqK0ZWbJkidLT07V69Wq98847g/YNhUIKBoMRGwAAmJhGPIykp6fr5Zdf1m9+8xv95je/kcfj0cqVK3X27NmoY3w+n1wuV3jzeDwjXSYAADDEZlmWlfBgm00VFRXKy8uLa9yKFSs0Z84c/fKXvxzw+VAopFAoFH4cDAbl8XgUCATkdDoTLRcAAIyiYDAol8t1w/N33GtGhsOyZct04sSJqM/b7XbZ7fZRrAgAAJhi5D4jjY2NSk9PN7FrAAAwxsR9ZaS7u1sXLlwIP25paVFjY6NSUlI0Z84clZSU6M9//rP+67/+S5L085//XPPmzdPixYvV29urV199VcePH9fvf//74XsVAABg3Io7jJw+fVp33313+PH3v/99SdLmzZtVVlam9vZ2tba2hp//29/+pn//93/Xn//8Z33lK19Rdna23n777Yi/AQAAJq8hLWAdLbEugAEAAGNHrOdvfpsGAAAYRRgBAABGEUYAAIBRhBEAAGAUYQQAABhFGAEAAEYRRgAAgFGEEQAAYBRhBAAAGEUYAQAARhFGAACAUYQRAABgFGEEAAAYRRgBAABGEUYAAIBRhBEAAGAUYQQAABhFGAEAAEYRRgAAgFGEEQAAYBRhBAAAGEUYAQAARhFGAACAUYQRAABgFGEEAAAYRRgBAABGEUYAAIBRhBEAAGAUYQQAABhFGAEAAEZNNV0AMNH09Vt6r+VTdXb1KjXZoWXzUjQlyWa6LAAYs+K+MlJbW6sNGzZo5syZstlsOnTo0A3HVFdXKycnR3a7XfPnz1dZWVkCpQJjX2Vzu+7afVz5r5xUUXmj8l85qbt2H1dlc7vp0gBgzIo7jPT09Oi2227TSy+9FFP/lpYWrV+/XnfffbcaGxtVXFysrVu36ujRo3EXC4xllc3t2rb/rNoDvRHt/kCvtu0/SyABgChslmVZCQ+22VRRUaG8vLyofZ544gn97ne/U3Nzc7jtwQcf1KVLl1RZWRnTfoLBoFwulwKBgJxOZ6LlAiOmr9/SXbuPXxdErrJJcrscOvHEPXxkA2DSiPX8PeILWOvr67Vq1aqItjVr1qi+vj7qmFAopGAwGLEBY9l7LZ9GDSKSZElqD/TqvZZPR68oABgnRjyM+P1+paWlRbSlpaUpGAzq888/H3CMz+eTy+UKbx6PZ6TLBIaksyt6EEmkHwBMJmPyq70lJSUKBALhra2tzXRJwKBSkx3D2g8AJpMR/2qv2+1WR0dHRFtHR4ecTqduuummAcfY7XbZ7faRLg0YNsvmpSjd5ZA/0KuBFmFdXTOybF7KaJcGAGPeiF8Z8Xq9OnbsWERbVVWVvF7vSO8aGDVTkmzasSFT0pXgca2rj3dsyGTxKgAMIO4w0t3drcbGRjU2Nkq68tXdxsZGtba2SrryEUtBQUG4/2OPPaY//elP+uEPf6gPPvhA//Ef/6E33nhD3/ve94bnFQBjxNqsdJU+lCO3K/KjGLfLodKHcrQ2K91QZQAwtsX91d7q6mrdfffd17Vv3rxZZWVleuSRR/TRRx+puro6Ysz3vvc9vf/++5o9e7Z+9KMf6ZFHHol5n3y1F+MJd2AFgCtiPX8P6T4jo4UwAgDA+DNm7jMCAAAwGMIIAAAwijACAACMIowAAACjCCMAAMAowggAADCKMAIAAIwijAAAAKMIIwAAwKgR/9Xe4XD1JrHBYNBwJQAAIFZXz9s3utn7uAgjXV1dkiSPx2O4EgAAEK+uri65XK6oz4+L36bp7+/XJ598ouTkZNlsw/eDY8FgUB6PR21tbfzmzQ0wV/FhvmLHXMWOuYodcxW7kZwry7LU1dWlmTNnKikp+sqQcXFlJCkpSbNnzx6xv+90OjlYY8RcxYf5ih1zFTvmKnbMVexGaq4GuyJyFQtYAQCAUYQRAABg1KQOI3a7XTt27JDdbjddypjHXMWH+YodcxU75ip2zFXsxsJcjYsFrAAAYOKa1FdGAACAeYQRAABgFGEEAAAYRRgBAABGTegwUltbqw0bNmjmzJmy2Ww6dOjQDcdUV1crJydHdrtd8+fPV1lZ2YjXORbEO1fV1dWy2WzXbX6/f3QKNsjn8+nb3/62kpOTlZqaqry8PJ0/f/6G4w4ePKiFCxfK4XDo1ltv1ZEjR0ahWrMSmauysrLrjiuHwzFKFZtTWlqq7Ozs8I2nvF6v3nrrrUHHTMZjSop/ribrMTWQXbt2yWazqbi4eNB+o31sTegw0tPTo9tuu00vvfRSTP1bWlq0fv163X333WpsbFRxcbG2bt2qo0ePjnCl5sU7V1edP39e7e3t4S01NXWEKhw7ampqVFhYqJMnT6qqqkqXL1/Wfffdp56enqhj6urqlJ+fry1btqihoUF5eXnKy8tTc3PzKFY++hKZK+nKnSCvPa4uXrw4ShWbM3v2bO3atUtnzpzR6dOndc8992jjxo06d+7cgP0n6zElxT9X0uQ8pr7s1KlT2rdvn7KzswftZ+TYsiYJSVZFRcWgfX74wx9aixcvjmh74IEHrDVr1oxgZWNPLHP1P//zP5Yk67PPPhuVmsayzs5OS5JVU1MTtc8///M/W+vXr49o+853vmP927/920iXN6bEMlevv/665XK5Rq+oMWzGjBnWq6++OuBzHFORBpsrjinL6urqshYsWGBVVVVZK1assIqKiqL2NXFsTegrI/Gqr6/XqlWrItrWrFmj+vp6QxWNfUuWLFF6erpWr16td955x3Q5RgQCAUlSSkpK1D4cW1fEMleS1N3drblz58rj8dzwf7wTUV9fn8rLy9XT0yOv1ztgH46pK2KZK4ljqrCwUOvXr7/umBmIiWNrXPxQ3mjx+/1KS0uLaEtLS1MwGNTnn3+um266yVBlY096erpefvll3XHHHQqFQnr11Ve1cuVKvfvuu8rJyTFd3qjp7+9XcXGx7rzzTmVlZUXtF+3YmgxrbK6Kda4yMjL02muvKTs7W4FAQD/96U+Vm5urc+fOjegPZo4FTU1N8nq96u3t1c0336yKigplZmYO2HeyH1PxzNVkPqYkqby8XGfPntWpU6di6m/i2CKMICEZGRnKyMgIP87NzdWHH36oF198Ub/85S8NVja6CgsL1dzcrBMnTpguZcyLda68Xm/E/3Bzc3O1aNEi7du3Tzt37hzpMo3KyMhQY2OjAoGAfv3rX2vz5s2qqamJepKdzOKZq8l8TLW1tamoqEhVVVVjetEuYeQabrdbHR0dEW0dHR1yOp1cFYnBsmXLJtVJefv27Tp8+LBqa2tv+L+raMeW2+0eyRLHjHjm6sumTZum22+/XRcuXBih6saO6dOna/78+ZKkpUuX6tSpU9qzZ4/27dt3Xd/JfkzFM1dfNpmOqTNnzqizszPiinVfX59qa2u1d+9ehUIhTZkyJWKMiWOLNSPX8Hq9OnbsWERbVVXVoJ9D4guNjY1KT083XcaIsyxL27dvV0VFhY4fP6558+bdcMxkPbYSmasv6+vrU1NT06Q4tr6sv79foVBowOcm6zEVzWBz9WWT6Zi699571dTUpMbGxvB2xx13aNOmTWpsbLwuiEiGjq0RWxo7BnR1dVkNDQ1WQ0ODJcn62c9+ZjU0NFgXL160LMuynnzySevhhx8O9//Tn/5kfeUrX7Eef/xx6w9/+IP10ksvWVOmTLEqKytNvYRRE+9cvfjii9ahQ4esP/7xj1ZTU5NVVFRkJSUlWW+//baplzBqtm3bZrlcLqu6utpqb28Pb3/961/DfR5++GHrySefDD9+5513rKlTp1o//elPrT/84Q/Wjh07rGnTpllNTU0mXsKoSWSunn32Wevo0aPWhx9+aJ05c8Z68MEHLYfDYZ07d87ESxg1Tz75pFVTU2O1tLRY//u//2s9+eSTls1ms37/+99blsUxda1452qyHlPRfPnbNGPh2JrQYeTq10+/vG3evNmyLMvavHmztWLFiuvGLFmyxJo+fbr1jW98w3r99ddHvW4T4p2r3bt3W9/85jcth8NhpaSkWCtXrrSOHz9upvhRNtA8SYo4VlasWBGeu6veeOMN61vf+pY1ffp0a/Hixdbvfve70S3cgETmqri42JozZ441ffp0Ky0tzbr//vuts2fPjn7xo+xf//Vfrblz51rTp0+3brnlFuvee+8Nn1wti2PqWvHO1WQ9pqL5chgZC8eWzbIsa+SuuwAAAAyONSMAAMAowggAADCKMAIAAIwijAAAAKMIIwAAwCjCCAAAMIowAgAAjCKMAAAAowgjAADAKMIIAAAwijACAACMIowAAACj/h+S/UE3M1jB4AAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the plot above, convince yourself that for our neuron we will not be able to make `L = 0` as we could with the single sample. That is, there is no line we can draw that will pass through all the points (that is how the loss would equal to 0). \n",
    "\n",
    "In the following code we will apply stochastic gradient descent (update weight after each sample in the trainging data) to the training data of 5 samples we created above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 weight: 0.19583167  Total Loss: 39.599796\n",
      "Epoch 2 weight: 0.28229229  Total Loss: 33.127512\n",
      "Epoch 3 weight: 0.36029822  Total Loss: 27.860079\n",
      "Epoch 4 weight: 0.43067622  Total Loss: 23.573291\n",
      "Epoch 5 weight: 0.49417220  Total Loss: 20.084655\n",
      "Epoch 6 weight: 0.55145913  Total Loss: 17.245631\n",
      "Epoch 7 weight: 0.60314417  Total Loss: 14.935316\n",
      "Epoch 8 weight: 0.64977510  Total Loss: 13.055305\n",
      "Epoch 9 weight: 0.69184616  Total Loss: 11.525502\n",
      "Epoch 10 weight: 0.72980323  Total Loss: 10.280714\n",
      "Epoch 11 weight: 0.76404861  Total Loss: 9.267882\n",
      "Epoch 12 weight: 0.79494525  Total Loss: 8.443818\n",
      "Epoch 13 weight: 0.82282062  Total Loss: 7.773374\n",
      "Epoch 14 weight: 0.84797015  Total Loss: 7.227942\n",
      "Epoch 15 weight: 0.87066040  Total Loss: 6.784238\n",
      "Epoch 16 weight: 0.89113184  Total Loss: 6.423314\n",
      "Epoch 17 weight: 0.90960145  Total Loss: 6.129747\n",
      "Epoch 18 weight: 0.92626498  Total Loss: 5.890987\n",
      "Epoch 19 weight: 0.94129905  Total Loss: 5.696820\n",
      "Epoch 20 weight: 0.95486298  Total Loss: 5.538932\n",
      "Epoch 21 weight: 0.96710055  Total Loss: 5.410560\n",
      "Epoch 22 weight: 0.97814144  Total Loss: 5.306200\n",
      "Epoch 23 weight: 0.98810268  Total Loss: 5.221371\n",
      "Epoch 24 weight: 0.99708985  Total Loss: 5.152429\n",
      "Epoch 25 weight: 1.00519819  Total Loss: 5.096408\n",
      "Epoch 26 weight: 1.01251365  Total Loss: 5.050896\n",
      "Epoch 27 weight: 1.01911375  Total Loss: 5.013929\n",
      "Epoch 28 weight: 1.02506846  Total Loss: 4.983909\n",
      "Epoch 29 weight: 1.03044087  Total Loss: 4.959538\n",
      "Epoch 30 weight: 1.03528793  Total Loss: 4.939758\n",
      "Epoch 31 weight: 1.03966102  Total Loss: 4.923711\n",
      "Epoch 32 weight: 1.04360647  Total Loss: 4.910695\n",
      "Epoch 33 weight: 1.04716611  Total Loss: 4.900143\n",
      "Epoch 34 weight: 1.05037767  Total Loss: 4.891593\n",
      "Epoch 35 weight: 1.05327518  Total Loss: 4.884668\n",
      "Epoch 36 weight: 1.05588936  Total Loss: 4.879062\n",
      "Epoch 37 weight: 1.05824790  Total Loss: 4.874528\n",
      "Epoch 38 weight: 1.06037581  Total Loss: 4.870862\n",
      "Epoch 39 weight: 1.06229564  Total Loss: 4.867901\n",
      "Epoch 40 weight: 1.06402773  Total Loss: 4.865512\n",
      "Epoch 41 weight: 1.06559045  Total Loss: 4.863586\n",
      "Epoch 42 weight: 1.06700036  Total Loss: 4.862035\n",
      "Epoch 43 weight: 1.06827239  Total Loss: 4.860788\n",
      "Epoch 44 weight: 1.06942004  Total Loss: 4.859787\n",
      "Epoch 45 weight: 1.07045546  Total Loss: 4.858984\n",
      "Epoch 46 weight: 1.07138964  Total Loss: 4.858342\n",
      "Epoch 47 weight: 1.07223246  Total Loss: 4.857830\n",
      "Epoch 48 weight: 1.07299287  Total Loss: 4.857422\n",
      "Epoch 49 weight: 1.07367892  Total Loss: 4.857098\n",
      "Epoch 50 weight: 1.07429788  Total Loss: 4.856841\n",
      "Epoch 51 weight: 1.07485632  Total Loss: 4.856639\n",
      "Epoch 52 weight: 1.07536014  Total Loss: 4.856481\n",
      "Epoch 53 weight: 1.07581471  Total Loss: 4.856358\n",
      "Epoch 54 weight: 1.07622482  Total Loss: 4.856262\n",
      "Epoch 55 weight: 1.07659482  Total Loss: 4.856189\n",
      "Epoch 56 weight: 1.07692865  Total Loss: 4.856133\n",
      "Epoch 57 weight: 1.07722983  Total Loss: 4.856092\n",
      "Epoch 58 weight: 1.07750156  Total Loss: 4.856061\n",
      "Epoch 59 weight: 1.07774672  Total Loss: 4.856039\n",
      "Epoch 60 weight: 1.07796791  Total Loss: 4.856024\n",
      "Epoch 61 weight: 1.07816747  Total Loss: 4.856014\n",
      "Epoch 62 weight: 1.07834751  Total Loss: 4.856008\n",
      "Epoch 63 weight: 1.07850995  Total Loss: 4.856005\n",
      "Epoch 64 weight: 1.07865650  Total Loss: 4.856004\n",
      "Epoch 65 weight: 1.07878872  Total Loss: 4.856005\n",
      "Epoch 66 weight: 1.07890801  Total Loss: 4.856007\n",
      "Epoch 67 weight: 1.07901564  Total Loss: 4.856011\n",
      "Epoch 68 weight: 1.07911274  Total Loss: 4.856014\n",
      "Epoch 69 weight: 1.07920035  Total Loss: 4.856018\n",
      "Epoch 70 weight: 1.07927939  Total Loss: 4.856023\n",
      "Epoch 71 weight: 1.07935070  Total Loss: 4.856027\n",
      "Epoch 72 weight: 1.07941504  Total Loss: 4.856031\n",
      "Epoch 73 weight: 1.07947309  Total Loss: 4.856036\n",
      "Epoch 74 weight: 1.07952546  Total Loss: 4.856040\n",
      "Epoch 75 weight: 1.07957271  Total Loss: 4.856043\n",
      "Epoch 76 weight: 1.07961534  Total Loss: 4.856047\n",
      "Epoch 77 weight: 1.07965380  Total Loss: 4.856051\n",
      "Epoch 78 weight: 1.07968850  Total Loss: 4.856054\n",
      "Epoch 79 weight: 1.07971980  Total Loss: 4.856057\n",
      "Epoch 80 weight: 1.07974805  Total Loss: 4.856060\n",
      "Epoch 81 weight: 1.07977353  Total Loss: 4.856062\n",
      "Epoch 82 weight: 1.07979652  Total Loss: 4.856064\n",
      "Epoch 83 weight: 1.07981727  Total Loss: 4.856067\n",
      "Epoch 84 weight: 1.07983598  Total Loss: 4.856069\n",
      "Epoch 85 weight: 1.07985287  Total Loss: 4.856070\n",
      "Epoch 86 weight: 1.07986810  Total Loss: 4.856072\n",
      "Epoch 87 weight: 1.07988184  Total Loss: 4.856074\n",
      "Epoch 88 weight: 1.07989424  Total Loss: 4.856075\n",
      "Epoch 89 weight: 1.07990543  Total Loss: 4.856076\n",
      "Epoch 90 weight: 1.07991552  Total Loss: 4.856077\n",
      "Epoch 91 weight: 1.07992463  Total Loss: 4.856079\n",
      "Epoch 92 weight: 1.07993285  Total Loss: 4.856079\n",
      "Epoch 93 weight: 1.07994026  Total Loss: 4.856080\n",
      "Epoch 94 weight: 1.07994695  Total Loss: 4.856081\n",
      "Epoch 95 weight: 1.07995298  Total Loss: 4.856082\n",
      "Epoch 96 weight: 1.07995842  Total Loss: 4.856082\n",
      "Epoch 97 weight: 1.07996333  Total Loss: 4.856083\n",
      "Epoch 98 weight: 1.07996777  Total Loss: 4.856084\n",
      "Epoch 99 weight: 1.07997176  Total Loss: 4.856084\n",
      "Epoch 100 weight: 1.07997537  Total Loss: 4.856084\n",
      "Epoch 101 weight: 1.07997862  Total Loss: 4.856085\n",
      "Epoch 102 weight: 1.07998156  Total Loss: 4.856085\n",
      "Epoch 103 weight: 1.07998421  Total Loss: 4.856086\n",
      "Epoch 104 weight: 1.07998660  Total Loss: 4.856086\n",
      "Epoch 105 weight: 1.07998876  Total Loss: 4.856086\n",
      "Epoch 106 weight: 1.07999070  Total Loss: 4.856086\n",
      "Epoch 107 weight: 1.07999246  Total Loss: 4.856087\n",
      "Epoch 108 weight: 1.07999404  Total Loss: 4.856087\n",
      "Epoch 109 weight: 1.07999547  Total Loss: 4.856087\n",
      "Epoch 110 weight: 1.07999676  Total Loss: 4.856087\n",
      "Epoch 111 weight: 1.07999792  Total Loss: 4.856087\n",
      "Epoch 112 weight: 1.07999897  Total Loss: 4.856087\n",
      "Epoch 113 weight: 1.07999992  Total Loss: 4.856087\n",
      "Epoch 114 weight: 1.08000077  Total Loss: 4.856088\n",
      "Epoch 115 weight: 1.08000154  Total Loss: 4.856088\n",
      "Epoch 116 weight: 1.08000224  Total Loss: 4.856088\n",
      "Epoch 117 weight: 1.08000286  Total Loss: 4.856088\n",
      "Epoch 118 weight: 1.08000343  Total Loss: 4.856088\n",
      "Epoch 119 weight: 1.08000394  Total Loss: 4.856088\n",
      "Epoch 120 weight: 1.08000440  Total Loss: 4.856088\n",
      "Epoch 121 weight: 1.08000481  Total Loss: 4.856088\n",
      "Epoch 122 weight: 1.08000519  Total Loss: 4.856088\n",
      "Epoch 123 weight: 1.08000553  Total Loss: 4.856088\n",
      "Epoch 124 weight: 1.08000583  Total Loss: 4.856088\n",
      "Epoch 125 weight: 1.08000611  Total Loss: 4.856088\n",
      "Epoch 126 weight: 1.08000636  Total Loss: 4.856088\n",
      "Epoch 127 weight: 1.08000658  Total Loss: 4.856088\n",
      "Epoch 128 weight: 1.08000678  Total Loss: 4.856088\n",
      "Epoch 129 weight: 1.08000697  Total Loss: 4.856088\n",
      "Epoch 130 weight: 1.08000713  Total Loss: 4.856088\n",
      "Epoch 131 weight: 1.08000728  Total Loss: 4.856088\n",
      "Epoch 132 weight: 1.08000741  Total Loss: 4.856088\n",
      "Epoch 133 weight: 1.08000753  Total Loss: 4.856088\n",
      "Epoch 134 weight: 1.08000764  Total Loss: 4.856088\n",
      "Epoch 135 weight: 1.08000774  Total Loss: 4.856088\n",
      "Epoch 136 weight: 1.08000783  Total Loss: 4.856088\n",
      "Epoch 137 weight: 1.08000791  Total Loss: 4.856088\n",
      "Epoch 138 weight: 1.08000798  Total Loss: 4.856088\n",
      "Epoch 139 weight: 1.08000805  Total Loss: 4.856088\n",
      "Epoch 140 weight: 1.08000811  Total Loss: 4.856088\n",
      "Epoch 141 weight: 1.08000816  Total Loss: 4.856088\n",
      "Epoch 142 weight: 1.08000821  Total Loss: 4.856088\n",
      "Epoch 143 weight: 1.08000825  Total Loss: 4.856088\n",
      "Epoch 144 weight: 1.08000829  Total Loss: 4.856088\n",
      "Epoch 145 weight: 1.08000832  Total Loss: 4.856088\n",
      "Epoch 146 weight: 1.08000836  Total Loss: 4.856088\n",
      "Epoch 147 weight: 1.08000838  Total Loss: 4.856088\n",
      "Epoch 148 weight: 1.08000841  Total Loss: 4.856088\n",
      "Epoch 149 weight: 1.08000843  Total Loss: 4.856088\n",
      "Epoch 150 weight: 1.08000845  Total Loss: 4.856088\n",
      "Epoch 151 weight: 1.08000847  Total Loss: 4.856088\n",
      "Epoch 152 weight: 1.08000849  Total Loss: 4.856089\n",
      "Epoch 153 weight: 1.08000851  Total Loss: 4.856089\n",
      "Epoch 154 weight: 1.08000852  Total Loss: 4.856089\n",
      "Epoch 155 weight: 1.08000853  Total Loss: 4.856089\n",
      "Epoch 156 weight: 1.08000854  Total Loss: 4.856089\n",
      "Epoch 157 weight: 1.08000855  Total Loss: 4.856089\n",
      "Epoch 158 weight: 1.08000856  Total Loss: 4.856089\n",
      "Epoch 159 weight: 1.08000857  Total Loss: 4.856089\n",
      "Epoch 160 weight: 1.08000858  Total Loss: 4.856089\n",
      "Epoch 161 weight: 1.08000859  Total Loss: 4.856089\n",
      "Epoch 162 weight: 1.08000859  Total Loss: 4.856089\n",
      "Epoch 163 weight: 1.08000860  Total Loss: 4.856089\n",
      "Epoch 164 weight: 1.08000860  Total Loss: 4.856089\n",
      "Epoch 165 weight: 1.08000861  Total Loss: 4.856089\n",
      "Epoch 166 weight: 1.08000861  Total Loss: 4.856089\n",
      "Epoch 167 weight: 1.08000861  Total Loss: 4.856089\n",
      "Epoch 168 weight: 1.08000862  Total Loss: 4.856089\n",
      "Epoch 169 weight: 1.08000862  Total Loss: 4.856089\n",
      "Epoch 170 weight: 1.08000862  Total Loss: 4.856089\n",
      "Epoch 171 weight: 1.08000863  Total Loss: 4.856089\n",
      "Epoch 172 weight: 1.08000863  Total Loss: 4.856089\n",
      "Epoch 173 weight: 1.08000863  Total Loss: 4.856089\n",
      "Epoch 174 weight: 1.08000863  Total Loss: 4.856089\n",
      "Epoch 175 weight: 1.08000863  Total Loss: 4.856089\n",
      "Epoch 176 weight: 1.08000864  Total Loss: 4.856089\n",
      "Epoch 177 weight: 1.08000864  Total Loss: 4.856089\n",
      "Epoch 178 weight: 1.08000864  Total Loss: 4.856089\n",
      "Epoch 179 weight: 1.08000864  Total Loss: 4.856089\n",
      "Epoch 180 weight: 1.08000864  Total Loss: 4.856089\n",
      "Epoch 181 weight: 1.08000864  Total Loss: 4.856089\n",
      "Epoch 182 weight: 1.08000864  Total Loss: 4.856089\n",
      "Epoch 183 weight: 1.08000864  Total Loss: 4.856089\n",
      "Epoch 184 weight: 1.08000864  Total Loss: 4.856089\n",
      "Epoch 185 weight: 1.08000864  Total Loss: 4.856089\n",
      "Epoch 186 weight: 1.08000864  Total Loss: 4.856089\n",
      "Epoch 187 weight: 1.08000864  Total Loss: 4.856089\n",
      "Epoch 188 weight: 1.08000864  Total Loss: 4.856089\n",
      "Epoch 189 weight: 1.08000865  Total Loss: 4.856089\n",
      "Epoch 190 weight: 1.08000865  Total Loss: 4.856089\n",
      "Epoch 191 weight: 1.08000865  Total Loss: 4.856089\n",
      "Epoch 192 weight: 1.08000865  Total Loss: 4.856089\n",
      "Epoch 193 weight: 1.08000865  Total Loss: 4.856089\n",
      "Epoch 194 weight: 1.08000865  Total Loss: 4.856089\n",
      "Epoch 195 weight: 1.08000865  Total Loss: 4.856089\n",
      "Epoch 196 weight: 1.08000865  Total Loss: 4.856089\n",
      "Epoch 197 weight: 1.08000865  Total Loss: 4.856089\n",
      "Epoch 198 weight: 1.08000865  Total Loss: 4.856089\n",
      "Epoch 199 weight: 1.08000865  Total Loss: 4.856089\n",
      "Epoch 200 weight: 1.08000865  Total Loss: 4.856089\n"
     ]
    }
   ],
   "source": [
    "w = 0.1 # initial value for our weight\n",
    "lr = 0.0013\n",
    "\n",
    "n_epochs = 200\n",
    "epoch_counter = 0\n",
    "\n",
    "for i in range(n_epochs):\n",
    "    epoch_counter = epoch_counter + 1 # use this to keep track of how many times we have gone through entire training data\n",
    "    Total_L = 0 # set the total_loss (sum of losses from each sample in training set) to 0 at start of each epoch\n",
    "    for x, y in zip(X, Y): # iterate through the training data one sample at a time\n",
    "        y_hat = w * x # make prediction for current sample\n",
    "        L = (y - y_hat)**2  # calculate loss for current sample\n",
    "        Total_L = Total_L + L # add the loss for current sample to total loss\n",
    "        dL_dw = -2 * (y - y_hat)*x   # calculate derivative for current sample\n",
    "        w = w - lr*dL_dw # update the weight using the derivative and loss for current sample\n",
    "    print(f\"Epoch {epoch_counter} weight: {w:.8f}  Total Loss: {Total_L:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare with scikit-learn\n",
    "\n",
    "Our neuron is really just performing linear regression since we have no activation function. So, let's perform linear regression on the data and see what value scikit-learn gives us for `w`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.07692308])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "\n",
    "X_arr = np.array(X).reshape(-1, 1)\n",
    "Y_arr = np.array(Y) \n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X_arr, Y_arr)\n",
    "\n",
    "lin_reg.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Batch Gradient Descent\n",
    "\n",
    "We will now implement batch gradient descent where we will only make an update to the weight after we have processed all of the samples. The recipe is as follows: \n",
    "1. make predictions for all samples in our training data\n",
    "2. calculate the average loss for all samples in our training data\n",
    "3. calculate the average value of the derivative for all samples in our training data\n",
    "4. update the value of the weight\n",
    "5. repeat until the loss is a minimum (that is, stops decreasing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 weight: 0.2981  AVG Loss: 8.3980\n",
      "Epoch 2 weight: 0.4561  AVG Loss: 5.6848\n",
      "Epoch 3 weight: 0.5820  AVG Loss: 3.9605\n",
      "Epoch 4 weight: 0.6823  AVG Loss: 2.8647\n",
      "Epoch 5 weight: 0.7624  AVG Loss: 2.1682\n",
      "Epoch 6 weight: 0.8262  AVG Loss: 1.7256\n",
      "Epoch 7 weight: 0.8770  AVG Loss: 1.4443\n",
      "Epoch 8 weight: 0.9176  AVG Loss: 1.2656\n",
      "Epoch 9 weight: 0.9499  AVG Loss: 1.1519\n",
      "Epoch 10 weight: 0.9756  AVG Loss: 1.0797\n",
      "Epoch 11 weight: 0.9962  AVG Loss: 1.0339\n",
      "Epoch 12 weight: 1.0126  AVG Loss: 1.0047\n",
      "Epoch 13 weight: 1.0256  AVG Loss: 0.9862\n",
      "Epoch 14 weight: 1.0360  AVG Loss: 0.9744\n",
      "Epoch 15 weight: 1.0443  AVG Loss: 0.9669\n",
      "Epoch 16 weight: 1.0509  AVG Loss: 0.9621\n",
      "Epoch 17 weight: 1.0562  AVG Loss: 0.9591\n",
      "Epoch 18 weight: 1.0604  AVG Loss: 0.9572\n",
      "Epoch 19 weight: 1.0638  AVG Loss: 0.9560\n",
      "Epoch 20 weight: 1.0664  AVG Loss: 0.9552\n",
      "Epoch 21 weight: 1.0686  AVG Loss: 0.9547\n",
      "Epoch 22 weight: 1.0702  AVG Loss: 0.9544\n",
      "Epoch 23 weight: 1.0716  AVG Loss: 0.9542\n",
      "Epoch 24 weight: 1.0727  AVG Loss: 0.9541\n",
      "Epoch 25 weight: 1.0735  AVG Loss: 0.9540\n",
      "Epoch 26 weight: 1.0742  AVG Loss: 0.9539\n",
      "Epoch 27 weight: 1.0748  AVG Loss: 0.9539\n",
      "Epoch 28 weight: 1.0752  AVG Loss: 0.9539\n",
      "Epoch 29 weight: 1.0756  AVG Loss: 0.9539\n",
      "Epoch 30 weight: 1.0758  AVG Loss: 0.9539\n",
      "Epoch 31 weight: 1.0761  AVG Loss: 0.9539\n",
      "Epoch 32 weight: 1.0762  AVG Loss: 0.9539\n",
      "Epoch 33 weight: 1.0764  AVG Loss: 0.9538\n",
      "Epoch 34 weight: 1.0765  AVG Loss: 0.9538\n",
      "Epoch 35 weight: 1.0766  AVG Loss: 0.9538\n",
      "Epoch 36 weight: 1.0766  AVG Loss: 0.9538\n",
      "Epoch 37 weight: 1.0767  AVG Loss: 0.9538\n",
      "Epoch 38 weight: 1.0767  AVG Loss: 0.9538\n",
      "Epoch 39 weight: 1.0768  AVG Loss: 0.9538\n",
      "Epoch 40 weight: 1.0768  AVG Loss: 0.9538\n",
      "Epoch 41 weight: 1.0768  AVG Loss: 0.9538\n",
      "Epoch 42 weight: 1.0769  AVG Loss: 0.9538\n",
      "Epoch 43 weight: 1.0769  AVG Loss: 0.9538\n",
      "Epoch 44 weight: 1.0769  AVG Loss: 0.9538\n",
      "Epoch 45 weight: 1.0769  AVG Loss: 0.9538\n",
      "Epoch 46 weight: 1.0769  AVG Loss: 0.9538\n",
      "Epoch 47 weight: 1.0769  AVG Loss: 0.9538\n",
      "Epoch 48 weight: 1.0769  AVG Loss: 0.9538\n",
      "Epoch 49 weight: 1.0769  AVG Loss: 0.9538\n",
      "Epoch 50 weight: 1.0769  AVG Loss: 0.9538\n",
      "Epoch 51 weight: 1.0769  AVG Loss: 0.9538\n",
      "Epoch 52 weight: 1.0769  AVG Loss: 0.9538\n",
      "Epoch 53 weight: 1.0769  AVG Loss: 0.9538\n",
      "Epoch 54 weight: 1.0769  AVG Loss: 0.9538\n",
      "Epoch 55 weight: 1.0769  AVG Loss: 0.9538\n",
      "Epoch 56 weight: 1.0769  AVG Loss: 0.9538\n",
      "Epoch 57 weight: 1.0769  AVG Loss: 0.9538\n",
      "Epoch 58 weight: 1.0769  AVG Loss: 0.9538\n",
      "Epoch 59 weight: 1.0769  AVG Loss: 0.9538\n",
      "Epoch 60 weight: 1.0769  AVG Loss: 0.9538\n",
      "Epoch 61 weight: 1.0769  AVG Loss: 0.9538\n",
      "Epoch 62 weight: 1.0769  AVG Loss: 0.9538\n",
      "Epoch 63 weight: 1.0769  AVG Loss: 0.9538\n",
      "Epoch 64 weight: 1.0769  AVG Loss: 0.9538\n",
      "Epoch 65 weight: 1.0769  AVG Loss: 0.9538\n",
      "Epoch 66 weight: 1.0769  AVG Loss: 0.9538\n",
      "Epoch 67 weight: 1.0769  AVG Loss: 0.9538\n",
      "Epoch 68 weight: 1.0769  AVG Loss: 0.9538\n",
      "Epoch 69 weight: 1.0769  AVG Loss: 0.9538\n",
      "Epoch 70 weight: 1.0769  AVG Loss: 0.9538\n",
      "Epoch 71 weight: 1.0769  AVG Loss: 0.9538\n",
      "Epoch 72 weight: 1.0769  AVG Loss: 0.9538\n",
      "Epoch 73 weight: 1.0769  AVG Loss: 0.9538\n",
      "Epoch 74 weight: 1.0769  AVG Loss: 0.9538\n",
      "Epoch 75 weight: 1.0769  AVG Loss: 0.9538\n",
      "Epoch 76 weight: 1.0769  AVG Loss: 0.9538\n",
      "Epoch 77 weight: 1.0769  AVG Loss: 0.9538\n",
      "Epoch 78 weight: 1.0769  AVG Loss: 0.9538\n",
      "Epoch 79 weight: 1.0769  AVG Loss: 0.9538\n",
      "Epoch 80 weight: 1.0769  AVG Loss: 0.9538\n",
      "Epoch 81 weight: 1.0769  AVG Loss: 0.9538\n",
      "Epoch 82 weight: 1.0769  AVG Loss: 0.9538\n",
      "Epoch 83 weight: 1.0769  AVG Loss: 0.9538\n",
      "Epoch 84 weight: 1.0769  AVG Loss: 0.9538\n",
      "Epoch 85 weight: 1.0769  AVG Loss: 0.9538\n",
      "Epoch 86 weight: 1.0769  AVG Loss: 0.9538\n",
      "Epoch 87 weight: 1.0769  AVG Loss: 0.9538\n",
      "Epoch 88 weight: 1.0769  AVG Loss: 0.9538\n",
      "Epoch 89 weight: 1.0769  AVG Loss: 0.9538\n",
      "Epoch 90 weight: 1.0769  AVG Loss: 0.9538\n",
      "Epoch 91 weight: 1.0769  AVG Loss: 0.9538\n",
      "Epoch 92 weight: 1.0769  AVG Loss: 0.9538\n",
      "Epoch 93 weight: 1.0769  AVG Loss: 0.9538\n",
      "Epoch 94 weight: 1.0769  AVG Loss: 0.9538\n",
      "Epoch 95 weight: 1.0769  AVG Loss: 0.9538\n",
      "Epoch 96 weight: 1.0769  AVG Loss: 0.9538\n",
      "Epoch 97 weight: 1.0769  AVG Loss: 0.9538\n",
      "Epoch 98 weight: 1.0769  AVG Loss: 0.9538\n",
      "Epoch 99 weight: 1.0769  AVG Loss: 0.9538\n",
      "Epoch 100 weight: 1.0769  AVG Loss: 0.9538\n",
      "Epoch 101 weight: 1.0769  AVG Loss: 0.9538\n",
      "Epoch 102 weight: 1.0769  AVG Loss: 0.9538\n",
      "Epoch 103 weight: 1.0769  AVG Loss: 0.9538\n",
      "Epoch 104 weight: 1.0769  AVG Loss: 0.9538\n",
      "Epoch 105 weight: 1.0769  AVG Loss: 0.9538\n",
      "Epoch 106 weight: 1.0769  AVG Loss: 0.9538\n",
      "Epoch 107 weight: 1.0769  AVG Loss: 0.9538\n",
      "Epoch 108 weight: 1.0769  AVG Loss: 0.9538\n",
      "Epoch 109 weight: 1.0769  AVG Loss: 0.9538\n",
      "Epoch 110 weight: 1.0769  AVG Loss: 0.9538\n",
      "Epoch 111 weight: 1.0769  AVG Loss: 0.9538\n",
      "Epoch 112 weight: 1.0769  AVG Loss: 0.9538\n",
      "Epoch 113 weight: 1.0769  AVG Loss: 0.9538\n",
      "Epoch 114 weight: 1.0769  AVG Loss: 0.9538\n",
      "Epoch 115 weight: 1.0769  AVG Loss: 0.9538\n",
      "Epoch 116 weight: 1.0769  AVG Loss: 0.9538\n",
      "Epoch 117 weight: 1.0769  AVG Loss: 0.9538\n",
      "Epoch 118 weight: 1.0769  AVG Loss: 0.9538\n",
      "Epoch 119 weight: 1.0769  AVG Loss: 0.9538\n",
      "Epoch 120 weight: 1.0769  AVG Loss: 0.9538\n",
      "Epoch 121 weight: 1.0769  AVG Loss: 0.9538\n",
      "Epoch 122 weight: 1.0769  AVG Loss: 0.9538\n",
      "Epoch 123 weight: 1.0769  AVG Loss: 0.9538\n",
      "Epoch 124 weight: 1.0769  AVG Loss: 0.9538\n",
      "Epoch 125 weight: 1.0769  AVG Loss: 0.9538\n",
      "Epoch 126 weight: 1.0769  AVG Loss: 0.9538\n",
      "Epoch 127 weight: 1.0769  AVG Loss: 0.9538\n",
      "Epoch 128 weight: 1.0769  AVG Loss: 0.9538\n",
      "Epoch 129 weight: 1.0769  AVG Loss: 0.9538\n",
      "Epoch 130 weight: 1.0769  AVG Loss: 0.9538\n",
      "Epoch 131 weight: 1.0769  AVG Loss: 0.9538\n",
      "Epoch 132 weight: 1.0769  AVG Loss: 0.9538\n",
      "Epoch 133 weight: 1.0769  AVG Loss: 0.9538\n",
      "Epoch 134 weight: 1.0769  AVG Loss: 0.9538\n",
      "Epoch 135 weight: 1.0769  AVG Loss: 0.9538\n",
      "Epoch 136 weight: 1.0769  AVG Loss: 0.9538\n",
      "Epoch 137 weight: 1.0769  AVG Loss: 0.9538\n",
      "Epoch 138 weight: 1.0769  AVG Loss: 0.9538\n",
      "Epoch 139 weight: 1.0769  AVG Loss: 0.9538\n",
      "Epoch 140 weight: 1.0769  AVG Loss: 0.9538\n",
      "Epoch 141 weight: 1.0769  AVG Loss: 0.9538\n",
      "Epoch 142 weight: 1.0769  AVG Loss: 0.9538\n",
      "Epoch 143 weight: 1.0769  AVG Loss: 0.9538\n",
      "Epoch 144 weight: 1.0769  AVG Loss: 0.9538\n",
      "Epoch 145 weight: 1.0769  AVG Loss: 0.9538\n",
      "Epoch 146 weight: 1.0769  AVG Loss: 0.9538\n",
      "Epoch 147 weight: 1.0769  AVG Loss: 0.9538\n",
      "Epoch 148 weight: 1.0769  AVG Loss: 0.9538\n",
      "Epoch 149 weight: 1.0769  AVG Loss: 0.9538\n",
      "Epoch 150 weight: 1.0769  AVG Loss: 0.9538\n"
     ]
    }
   ],
   "source": [
    "X_new = np.array([1, 2, 3, 3, 4]).reshape(-1, 1) \n",
    "Y_new = np.array([2, 1, 4, 2, 5]).reshape(-1, 1) \n",
    "\n",
    "w = 0.1 \n",
    "lr = 0.013 \n",
    "\n",
    "n_epochs = 150\n",
    "\n",
    "for i in range(n_epochs):\n",
    "    Y_hat = w*X_new # make predictions for all samples (remember X_new is an array with 5 samples)\n",
    "    sample_losses = (Y_new - Y_hat)**2 # calculate the loss for all 5 samples (result is an array with 5 values)\n",
    "    avg_loss = sample_losses.mean() # calculate the average loss for the training data\n",
    "    dLoss_dw = (-2 * np.multiply((Y_new - Y_hat), X_new)).mean() # calculate the average of the derivatives\n",
    "    w = w - lr*dLoss_dw # update the weight using the average value of the derivative\n",
    "    print(f\"Epoch {i + 1} weight: {w:.4f}  AVG Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "11938c6bc6919ae2720b4d5011047913343b08a43b18698fd82dedb0d4417594"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
