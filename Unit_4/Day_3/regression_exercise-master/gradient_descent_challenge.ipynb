{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Goal: Train the 2nd order polynomial predictor using gradient descent. Optimize the stepsizes and compare against scikit-learn's implementation.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORT PACKAGES\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pylab import rcParams\n",
    "rcParams['figure.figsize'] = 12, 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task I\n",
    "Download the data from [here](https://drive.google.com/file/d/0Bz9_0VdXvv9bUUNlUTVrMF9VcVU/view?usp=sharing&resourcekey=0-16O9Fc5eaJH99-M7AHqHOg)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = ''\n",
    "nb = pd.read_csv(data_path+'nba_games_2013_2015.csv', delimiter=';')\n",
    "x = nb[['AST','REB','STL']]\n",
    "y = nb['PTS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SEASON_ID</th>\n",
       "      <th>TEAM_ID</th>\n",
       "      <th>TEAM_ABBREVIATION</th>\n",
       "      <th>TEAM_NAME</th>\n",
       "      <th>GAME_ID</th>\n",
       "      <th>GAME_DATE</th>\n",
       "      <th>MATCHUP</th>\n",
       "      <th>WL</th>\n",
       "      <th>MIN</th>\n",
       "      <th>PTS</th>\n",
       "      <th>...</th>\n",
       "      <th>FT_PCT</th>\n",
       "      <th>OREB</th>\n",
       "      <th>DREB</th>\n",
       "      <th>REB</th>\n",
       "      <th>AST</th>\n",
       "      <th>STL</th>\n",
       "      <th>BLK</th>\n",
       "      <th>TOV</th>\n",
       "      <th>PF</th>\n",
       "      <th>PLUS_MINUS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22015</td>\n",
       "      <td>1610612750</td>\n",
       "      <td>MIN</td>\n",
       "      <td>Minnesota Timberwolves</td>\n",
       "      <td>21501226</td>\n",
       "      <td>2016-04-13</td>\n",
       "      <td>MIN vs. NOP</td>\n",
       "      <td>W</td>\n",
       "      <td>240</td>\n",
       "      <td>144</td>\n",
       "      <td>...</td>\n",
       "      <td>0.826</td>\n",
       "      <td>5</td>\n",
       "      <td>38</td>\n",
       "      <td>43</td>\n",
       "      <td>41</td>\n",
       "      <td>14</td>\n",
       "      <td>8</td>\n",
       "      <td>13</td>\n",
       "      <td>20</td>\n",
       "      <td>35.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>22015</td>\n",
       "      <td>1610612749</td>\n",
       "      <td>MIL</td>\n",
       "      <td>Milwaukee Bucks</td>\n",
       "      <td>21501225</td>\n",
       "      <td>2016-04-13</td>\n",
       "      <td>MIL vs. IND</td>\n",
       "      <td>L</td>\n",
       "      <td>240</td>\n",
       "      <td>92</td>\n",
       "      <td>...</td>\n",
       "      <td>0.846</td>\n",
       "      <td>7</td>\n",
       "      <td>36</td>\n",
       "      <td>43</td>\n",
       "      <td>23</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>-5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22015</td>\n",
       "      <td>1610612738</td>\n",
       "      <td>BOS</td>\n",
       "      <td>Boston Celtics</td>\n",
       "      <td>21501217</td>\n",
       "      <td>2016-04-13</td>\n",
       "      <td>BOS vs. MIA</td>\n",
       "      <td>W</td>\n",
       "      <td>240</td>\n",
       "      <td>98</td>\n",
       "      <td>...</td>\n",
       "      <td>0.864</td>\n",
       "      <td>10</td>\n",
       "      <td>29</td>\n",
       "      <td>39</td>\n",
       "      <td>20</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>20</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>22015</td>\n",
       "      <td>1610612747</td>\n",
       "      <td>LAL</td>\n",
       "      <td>Los Angeles Lakers</td>\n",
       "      <td>21501228</td>\n",
       "      <td>2016-04-13</td>\n",
       "      <td>LAL vs. UTA</td>\n",
       "      <td>W</td>\n",
       "      <td>239</td>\n",
       "      <td>101</td>\n",
       "      <td>...</td>\n",
       "      <td>0.867</td>\n",
       "      <td>8</td>\n",
       "      <td>39</td>\n",
       "      <td>47</td>\n",
       "      <td>19</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>17</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>22015</td>\n",
       "      <td>1610612739</td>\n",
       "      <td>CLE</td>\n",
       "      <td>Cleveland Cavaliers</td>\n",
       "      <td>21501220</td>\n",
       "      <td>2016-04-13</td>\n",
       "      <td>CLE vs. DET</td>\n",
       "      <td>L</td>\n",
       "      <td>265</td>\n",
       "      <td>110</td>\n",
       "      <td>...</td>\n",
       "      <td>0.733</td>\n",
       "      <td>8</td>\n",
       "      <td>35</td>\n",
       "      <td>43</td>\n",
       "      <td>21</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>23</td>\n",
       "      <td>-2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   SEASON_ID     TEAM_ID TEAM_ABBREVIATION               TEAM_NAME   GAME_ID  \\\n",
       "0      22015  1610612750               MIN  Minnesota Timberwolves  21501226   \n",
       "1      22015  1610612749               MIL         Milwaukee Bucks  21501225   \n",
       "2      22015  1610612738               BOS          Boston Celtics  21501217   \n",
       "3      22015  1610612747               LAL      Los Angeles Lakers  21501228   \n",
       "4      22015  1610612739               CLE     Cleveland Cavaliers  21501220   \n",
       "\n",
       "    GAME_DATE      MATCHUP WL  MIN  PTS  ...  FT_PCT  OREB  DREB  REB  AST  \\\n",
       "0  2016-04-13  MIN vs. NOP  W  240  144  ...   0.826     5    38   43   41   \n",
       "1  2016-04-13  MIL vs. IND  L  240   92  ...   0.846     7    36   43   23   \n",
       "2  2016-04-13  BOS vs. MIA  W  240   98  ...   0.864    10    29   39   20   \n",
       "3  2016-04-13  LAL vs. UTA  W  239  101  ...   0.867     8    39   47   19   \n",
       "4  2016-04-13  CLE vs. DET  L  265  110  ...   0.733     8    35   43   21   \n",
       "\n",
       "   STL  BLK  TOV  PF  PLUS_MINUS  \n",
       "0   14    8   13  20        35.0  \n",
       "1    8    3   15  15        -5.0  \n",
       "2    7    3    7  20        10.0  \n",
       "3    6    3   13  17         5.0  \n",
       "4    4    7   10  23        -2.0  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7380 entries, 0 to 7379\n",
      "Data columns (total 28 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   SEASON_ID          7380 non-null   int64  \n",
      " 1   TEAM_ID            7380 non-null   int64  \n",
      " 2   TEAM_ABBREVIATION  7380 non-null   object \n",
      " 3   TEAM_NAME          7380 non-null   object \n",
      " 4   GAME_ID            7380 non-null   int64  \n",
      " 5   GAME_DATE          7380 non-null   object \n",
      " 6   MATCHUP            7380 non-null   object \n",
      " 7   WL                 7380 non-null   object \n",
      " 8   MIN                7380 non-null   int64  \n",
      " 9   PTS                7380 non-null   int64  \n",
      " 10  FGM                7380 non-null   int64  \n",
      " 11  FGA                7380 non-null   int64  \n",
      " 12  FG_PCT             7380 non-null   float64\n",
      " 13  FG3M               7380 non-null   int64  \n",
      " 14  FG3A               7380 non-null   int64  \n",
      " 15  FG3_PCT            7380 non-null   float64\n",
      " 16  FTM                7380 non-null   int64  \n",
      " 17  FTA                7380 non-null   int64  \n",
      " 18  FT_PCT             7380 non-null   float64\n",
      " 19  OREB               7380 non-null   int64  \n",
      " 20  DREB               7380 non-null   int64  \n",
      " 21  REB                7380 non-null   int64  \n",
      " 22  AST                7380 non-null   int64  \n",
      " 23  STL                7380 non-null   int64  \n",
      " 24  BLK                7380 non-null   int64  \n",
      " 25  TOV                7380 non-null   int64  \n",
      " 26  PF                 7380 non-null   int64  \n",
      " 27  PLUS_MINUS         7380 non-null   float64\n",
      "dtypes: float64(4), int64(19), object(5)\n",
      "memory usage: 1.6+ MB\n"
     ]
    }
   ],
   "source": [
    "nb.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### TASK II\n",
    "Create a function `psi(x)`, which transforms features AST (assists), REB (rebounds) and STL (steals) into 2nd order polynomial features (add each feature squared and each pair of features multiplied with every other)\n",
    "\n",
    "**Input:** DataFrame x from above. It contains columns AST, REB, STL\n",
    "\n",
    "**Output:** DataFrame with columns: AST, REB, STL, 1, AST^2, REB^2, STL^2, ASTSTL, REBSTL, ASTREB. The number of rows should be the same as Input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def psi(x):\n",
    "    poly = PolynomialFeatures(2)\n",
    "    Xpoly = poly.fit_transform(x)\n",
    "    return Xpoly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task III\n",
    "Create a transformed data matrix **X**, where each **x** is mapped to psi(x).\n",
    "\n",
    "HINT: We need to apply our function from Task II to matrix (DataFrame) x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = psi(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task IV\n",
    "Create a function `p2(x,w)`, which outputs the value of the polynomial ata given row of `x` for given parameters `w`.\n",
    "\n",
    "Inputs:\n",
    "- x: DataFrame from above\n",
    "- w: vector which represents beta coeficients for each column of X from the Task III.\n",
    "\n",
    "Ouputs:\n",
    "- Series of the same length as DataFrame x. Each value is a dot product of particular row in DataFrame and coeficients `w`.\n",
    "\n",
    "\n",
    "HINT:\n",
    "- length of w needs to be the same as number of columns of the dataframe that is returned from function `psi(x)`\n",
    "\n",
    "Example Input:\n",
    "\n",
    "`p2(x, [0.06, 0.05,0.03,0.01,0.02,0.02,0.04, 0.03,0.02,0.01])`\n",
    "\n",
    "Example Output:\n",
    "\n",
    "```\n",
    "0       130.37\n",
    "1        76.19\n",
    "2        61.21\n",
    "3        74.51\n",
    "4        64.97\n",
    "         ...  \n",
    "7375     63.01\n",
    "7376     79.59\n",
    "7377     97.25\n",
    "7378     78.85\n",
    "7379     61.53\n",
    "Length: 7380, dtype: float64\n",
    "\n",
    "```\n",
    "\n",
    "Our columns in the DataFrame **X** that is the output of `psi(x)` were in this order: `AST, REB, STL, 1, AST^2, REB^2, STL^2, ASTSTL, REBSTL, ASTREB`. If your columns are in different order the result can be different even for the **w**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def p2(x,w):\n",
    "    return x@w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task V\n",
    "Create a function `Loss(x,y,w)`, which computes the squared loss of predicting **y** from **x** by `p2(x,w)` using parameters **w**. We have specified **y** as the variable PTS above. We will predict scored points (PTS) based on assists, rebounds and steals.\n",
    "\n",
    "\n",
    "HINTS: \n",
    "- Output of `p2(x,w)` represents our predictions. `y_pred = p2(x,w)`\n",
    "- Loss can be computed as:\n",
    "\n",
    "```\n",
    "np.mean((y_pred - y)**2)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Loss(x,y,w):\n",
    "    y_pred = p2(x,w)\n",
    "    return np.mean((y_pred - y)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task VI\n",
    "Code up the gradient descent. It should input **x**, target variable **y**, stepsize **alpha**, coeficients **w** and **maximum number of iterations**.\n",
    "\n",
    "Steps:\n",
    "1. transform input `x`\n",
    "2. compute initial loss for given, x,y and w and append it to the empty list.\n",
    "3. Inside the for loop, update each element of **w**, **w[i]**, using gradient descent.\n",
    "\n",
    "HINT: `w[i] = w[i] - alpha * (1.0 / m) * errors_i.sum()` where `errors_i = (y_pred - y) * X.iloc[:, i]`. We are scaling the errors by multiplicating with values that are relevant for coeficients `w[i]` (column `i` of DataFrame `X`, output of `psi(x)`).\n",
    "\n",
    "4. compute new loss using updated `w` and append to the list that we created in the step 2.\n",
    "5. repeat steps 3 and 4 for max number of iterations times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(x,y,alpha,w,max_iterations):\n",
    "    \n",
    "    past_costs = []\n",
    "    past_costs.append(Loss(x,y,w)) # compute initial loss for given, x,y and w and append it to the empty list for past_costs\n",
    "    for i in range(max_iterations): \n",
    "        \n",
    "        error = y - p2(x,w) # actual value of the target minus predicted value of the target gives us the error\n",
    "        error = error.reshape(-1,1) # reshape error so that it is shape (7380,1)\n",
    "        grad = -(1.0 / y.size) * (error*x).sum() # gradient is the derivative of the loss function\n",
    "        w = w - alpha * grad # new coefficents are calculated by stepping 'downhill' in the direction opposite the gradient \n",
    "            \n",
    "        past_costs.append(Loss(x,y,w)) # compute loss for new w and append it to the list\n",
    "    \n",
    "    return past_costs, w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task VII\n",
    "Choose an arbitrary **w** and **alpha** and run gradient descent for 100 iterations. How does the loss behave? Does it converge to something?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the data as a pandas data frame\n",
    "\n",
    "data_path = ''\n",
    "nb = pd.read_csv(data_path+'nba_games_2013_2015.csv', delimiter=';')\n",
    "x = nb[['AST','REB','STL']] # features\n",
    "y = nb['PTS'] # target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function psi(x) transform features AST (assists), REB (rebounds) and STL (steals) into 2nd order polynomial features to attempt to find a function that will describe their relationship with PTS (target)\n",
    "\n",
    "def psi(x):\n",
    "    poly = PolynomialFeatures(2)\n",
    "    Xpoly = poly.fit_transform(x)\n",
    "    return Xpoly\n",
    "\n",
    "X = psi(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function p2(x,w), which outputs the value of the polynomial ata given row of `x` for given parameters `w`. This is the PTS prediction\n",
    "\n",
    "def p2(x,w):\n",
    "    return x@w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function `Loss(x,y,w)`, which computes the squared loss of predicting **y** from **x** by `p2(x,w)` using parameters **w**\n",
    "\n",
    "def Loss(x,y,w):\n",
    "    y_pred = p2(x,w)\n",
    "    return np.mean((y_pred - y)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function for gradient descent that returns the derivative of the loss function for a list of coefficients w, then adjusts the coefficients and repeats this for max_iterations\n",
    "\n",
    "def gradient_descent(x,y,alpha,w,max_iterations):\n",
    "    \n",
    "    past_costs = []\n",
    "    past_costs.append(Loss(x,y,w)) # compute initial loss for given, x,y and w and append it to the empty list for past_costs\n",
    "    for i in range(max_iterations): \n",
    "        \n",
    "        error = y - p2(x,w) # actual value of the target minus predicted value of the target gives us the error\n",
    "        error = error.reshape(-1,1) # reshape error so that it is shape (7380,1)\n",
    "        grad = -(1.0 / y.size) * (error*x).sum() # gradient is the derivative of the loss function\n",
    "        w = w - alpha * grad # new coefficents are calculated by stepping 'downhill' in the direction opposite the gradient \n",
    "            \n",
    "        past_costs.append(Loss(x,y,w)) # compute loss for new w and append it to the list\n",
    "    \n",
    "    return past_costs, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = [0.06,0.05,0.03,0.01,0.02,0.02,0.04, 0.03,0.02,0.01]\n",
    "alpha = .0000001\n",
    "max_iterations = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([524.5718913821138,\n",
       "  501.9157970055969,\n",
       "  490.18224285903807,\n",
       "  484.10545439663974,\n",
       "  480.95829561120325,\n",
       "  479.3283872340995,\n",
       "  478.48426035685196,\n",
       "  478.04708843056363,\n",
       "  477.82067780641944,\n",
       "  477.70342011523906,\n",
       "  477.6426925491503,\n",
       "  477.6112418421992,\n",
       "  477.5949535726138,\n",
       "  477.5865179044902,\n",
       "  477.5821490857943,\n",
       "  477.5798864817518,\n",
       "  477.5787146828262,\n",
       "  477.5781078100832,\n",
       "  477.5777935116842,\n",
       "  477.5776307370584,\n",
       "  477.57754643634655,\n",
       "  477.57750277714604,\n",
       "  477.5774801661176,\n",
       "  477.5774684559031,\n",
       "  477.5774623912022,\n",
       "  477.57745925030343,\n",
       "  477.5774576236372,\n",
       "  477.57745678118937,\n",
       "  477.5774563448871,\n",
       "  477.57745611892676,\n",
       "  477.5774560019023,\n",
       "  477.57745594129557,\n",
       "  477.5774559099074,\n",
       "  477.5774558936516,\n",
       "  477.5774558852327,\n",
       "  477.5774558808726,\n",
       "  477.5774558786145,\n",
       "  477.577455877445,\n",
       "  477.57745587683934,\n",
       "  477.5774558765257,\n",
       "  477.57745587636316,\n",
       "  477.57745587627903,\n",
       "  477.5774558762355,\n",
       "  477.57745587621287,\n",
       "  477.57745587620127,\n",
       "  477.5774558761952,\n",
       "  477.57745587619206,\n",
       "  477.5774558761904,\n",
       "  477.57745587618956,\n",
       "  477.57745587618916,\n",
       "  477.5774558761889,\n",
       "  477.5774558761888,\n",
       "  477.57745587618876,\n",
       "  477.5774558761887,\n",
       "  477.57745587618876,\n",
       "  477.57745587618865,\n",
       "  477.5774558761887,\n",
       "  477.57745587618865,\n",
       "  477.57745587618865,\n",
       "  477.57745587618865,\n",
       "  477.57745587618865,\n",
       "  477.57745587618865,\n",
       "  477.5774558761887,\n",
       "  477.5774558761887,\n",
       "  477.5774558761887,\n",
       "  477.57745587618865,\n",
       "  477.57745587618865,\n",
       "  477.57745587618865,\n",
       "  477.5774558761887,\n",
       "  477.57745587618865,\n",
       "  477.57745587618865,\n",
       "  477.5774558761887,\n",
       "  477.5774558761887,\n",
       "  477.5774558761887,\n",
       "  477.57745587618876,\n",
       "  477.57745587618865,\n",
       "  477.5774558761887,\n",
       "  477.57745587618865,\n",
       "  477.57745587618865,\n",
       "  477.5774558761887,\n",
       "  477.57745587618865,\n",
       "  477.57745587618865,\n",
       "  477.57745587618876,\n",
       "  477.57745587618865,\n",
       "  477.57745587618865,\n",
       "  477.57745587618865,\n",
       "  477.5774558761887,\n",
       "  477.5774558761887,\n",
       "  477.5774558761887,\n",
       "  477.57745587618865,\n",
       "  477.5774558761887,\n",
       "  477.57745587618865,\n",
       "  477.57745587618865,\n",
       "  477.57745587618865,\n",
       "  477.57745587618865,\n",
       "  477.57745587618865,\n",
       "  477.57745587618865,\n",
       "  477.5774558761887,\n",
       "  477.5774558761887,\n",
       "  477.57745587618865,\n",
       "  477.57745587618865],\n",
       " array([0.05834689, 0.04834689, 0.02834689, 0.00834689, 0.01834689,\n",
       "        0.01834689, 0.03834689, 0.02834689, 0.01834689, 0.00834689]))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient_descent(X,y.values,alpha,w,max_iterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task VIII\n",
    "Can you find which **alpha** from `[1, 0.1, 0.01, 0.001, 0.0001, 0.00001, 0.000001, 0.0000001, 0.00000001]` has the smallest loss after 100 iterations for a given **w**?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python39\\site-packages\\numpy\\core\\_methods.py:180: RuntimeWarning: overflow encountered in reduce\n",
      "  ret = umr_sum(arr, axis, dtype, out, keepdims, where=where)\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_7388\\3328901806.py:5: RuntimeWarning: overflow encountered in square\n",
      "  return np.mean((y_pred - y)**2)\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_7388\\915277839.py:11: RuntimeWarning: overflow encountered in multiply\n",
      "  grad = -(1.0 / y.size) * (error*x).sum() # gradient is the derivative of the loss function\n",
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python39\\site-packages\\numpy\\core\\_methods.py:48: RuntimeWarning: overflow encountered in reduce\n",
      "  return umr_sum(a, axis, dtype, out, keepdims, initial, where)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss with alpha 1:\n",
      " [524.5718913821138, 1.3897199768073922e+16, 4.1096814828085395e+30, 1.2153154716060302e+45, 3.5939322833253174e+59, 1.0627980601661458e+74, 3.1429076221987286e+88, 9.294209965090328e+102, 2.748484819122767e+117, 8.127822406984853e+131, 2.4035605588889487e+146, 7.107811995599032e+160, 2.1019229649921927e+175, 6.215808962725982e+189, 1.838139727506528e+204, 5.435748875325793e+218, 1.6074602704815697e+233, 4.753583324840064e+247, 1.4057301969539766e+262, 4.1570269238832646e+276, 1.2293164707804968e+291, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan] \n",
      " The weights after 100 iterations:\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      "\n",
      "The loss with alpha 0.1:\n",
      " [524.5718913821138, 138971852215961.12, 4.1096728794129634e+26, 1.2153116553123621e+39, 3.593917235935943e+51, 1.0627924978994528e+64, 3.142887883719455e+76, 9.294141866030608e+88, 2.7484618039786736e+101, 8.127745839063601e+113, 2.403535400375105e+126, 7.1077301569778e+138, 2.101896563559136e+151, 6.215724382227e+163, 1.8381127913540274e+176, 5.435663530062724e+188, 1.6074333496307832e+201, 4.7534987388658794e+213, 1.4057037117955523e+226, 4.1569442507672617e+238, 1.2292907359485042e+251, 3.635256145688977e+263, 1.0750172321580945e+276, 3.179038843816664e+288, 9.401047414102221e+300, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan] \n",
      " The weights after 100 iterations:\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      "\n",
      "The loss with alpha 0.01:\n",
      " [524.5718913821138, 1389703976149.1985, 4.109586846200142e+22, 1.215273492924938e+33, 3.593766765074108e+43, 1.0627368766734817e+54, 3.1426905051764474e+64, 9.293460900915626e+74, 2.7482316624747716e+85, 8.126980197317795e+95, 2.4032838289956886e+106, 7.106911820233548e+116, 2.1016325667069555e+127, 6.214878638100396e+137, 1.8378434507625488e+148, 5.434810148671206e+158, 1.6071641651438886e+169, 4.752652959467557e+179, 1.405438886892644e+190, 4.156117607651548e+200, 1.2290334165167194e+211, 3.6344571581271854e+221, 1.0747697057496762e+232, 3.1782735911859726e+242, 9.398686031426806e+252, 2.7793484916562833e+263, 8.218997860171495e+273, 2.4304949893220377e+284, 7.18737976772788e+294, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan] \n",
      " The weights after 100 iterations:\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      "\n",
      "The loss with alpha 0.001:\n",
      " [524.5718913821138, 13895585627.84417, 4.108726588363171e+18, 1.2148919239718689e+27, 3.592262359613616e+35, 1.0621808084877516e+44, 3.1407173446013143e+52, 9.286653797410687e+60, 2.745931240874208e+69, 8.119327525390591e+77, 2.400769490629256e+86, 7.098733397700924e+94, 2.098994345284954e+103, 6.206427280344291e+111, 1.8351521371522563e+120, 5.426283454831154e+128, 1.6044747209822958e+137, 4.7442031948758956e+145, 1.4027933042467048e+154, 4.147859974810514e+162, 1.2264631088950038e+171, 3.626476705133022e+179, 1.0722975030795098e+188, 3.1706309694008504e+196, 9.375104124790974e+204, 2.77208474271863e+213, 8.196659704816605e+221, 2.423635514499984e+230, 7.166344973055129e+238, 2.1189861250002265e+247, 6.265540125162681e+255, 1.8526309633112566e+264, 5.477965853950187e+272, 1.6197564701936058e+281, 4.7893891504309954e+289, 1.4161541476370436e+298, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan] \n",
      " The weights after 100 iterations:\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      "\n",
      "The loss with alpha 0.0001:\n",
      " [524.5718913821138, 138810910.3072161, 410013143624554.44, 1.2110817223067107e+21, 3.5772485855969507e+27, 1.05663451173071e+34, 3.121048103494001e+40, 9.21883693574268e+46, 2.723026099875582e+53, 8.043174201135028e+59, 2.3757631714495867e+66, 7.017441753306421e+72, 2.0727860989191716e+79, 6.122519235515131e+85, 1.8084471817327898e+92, 5.341724677884328e+98, 1.5778189610701352e+105, 4.660503534035471e+111, 1.37660236862825e+118, 4.0661573743556895e+124, 1.2010465890380903e+131, 3.547607178555424e+137, 1.047879142092034e+144, 3.0951868151272027e+150, 9.142448814670524e+156, 2.7004628580209095e+163, 7.976527728378938e+169, 2.356077381794725e+176, 6.959294592877741e+182, 2.0556108048354818e+189, 6.071787484439682e+195, 1.7934622239519189e+202, 5.297462661507147e+208, 1.5647450096955438e+215, 4.621886177996248e+221, 1.3651957162342495e+228, 4.0324648246364884e+234, 1.1910946077961801e+241, 3.51821138290779e+247, 1.039196320242266e+254, 3.0695398157472494e+260, 9.066693652515151e+266, 2.6780865772398057e+273, 7.910433494355933e+279, 2.336554710382876e+286, 6.901629245107403e+292, 2.0385778268002367e+299, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, nan, nan, nan, nan, nan, nan, nan] \n",
      " The weights after 100 iterations:\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      "\n",
      "The loss with alpha 1e-05:\n",
      " [524.5718913821138, 1374081.899504824, 40149197148.18456, 1173524258347742.2, 3.4301039600568254e+19, 1.0025879817233303e+24, 2.930472874295646e+28, 8.565503899439719e+32, 2.503618364628317e+37, 7.317847250194083e+41, 2.138939749514227e+46, 6.251925047944453e+50, 1.8273804493086905e+55, 5.341265739603762e+59, 1.561203071416119e+64, 4.563253635045568e+68, 1.3337972566802872e+73, 3.8985672596962617e+77, 1.1395155149894562e+82, 3.330699517039631e+86, 9.73532973169802e+90, 2.8455477445507344e+95, 8.31727552088313e+99, 2.4310634823385726e+104, 7.105775972337412e+108, 2.0769532567070853e+113, 6.07074420491079e+117, 1.7744229477695696e+122, 5.1864758113581576e+126, 1.515959392636106e+131, 4.4310105044524056e+135, 1.2951438004171196e+140, 3.785586746123495e+144, 1.1064923453141246e+149, 3.2341758156578546e+153, 9.453199790204297e+157, 2.7630837458148995e+162, 8.076240802926582e+166, 2.3606112411775484e+171, 6.899850521983581e+175, 2.0167631330082436e+180, 5.8948139843063454e+184, 1.7230001550921645e+189, 5.036171696598433e+193, 1.4720268760662088e+198, 4.3025997809502786e+202, 1.2576105216574016e+207, 3.67588040883061e+211, 1.0744261873872636e+216, 3.140449371993504e+220, 9.179245976903584e+224, 2.6830095544897984e+229, 7.8421912732224445e+233, 2.29220070658679e+238, 6.699892792997354e+242, 1.958317319625093e+247, 5.72398222304678e+251, 1.6730675954001166e+256, 4.8902233964101705e+260, 1.4293675242140054e+265, 4.177910401347871e+269, 1.2211649576471952e+274, 3.569353362160121e+278, 1.0432893069999584e+283, 3.049439121493249e+287, 8.913231347528695e+291, 2.6052559139355832e+296, 7.614924500953532e+300, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf, inf] \n",
      " The weights after 100 iterations:\n",
      " [3.22896666e+220 3.22896666e+220 3.22896666e+220 3.22896666e+220\n",
      " 3.22896666e+220 3.22896666e+220 3.22896666e+220 3.22896666e+220\n",
      " 3.22896666e+220 3.22896666e+220]\n",
      "\n",
      "The loss with alpha 1e-06:\n",
      " [524.5718913821138, 12805.492193535974, 3234423.897756878, 848352289.3108423, 222545684663.3101, 58379767526996.02, 1.531459605213474e+16, 4.0174338161506074e+18, 1.0538818269973997e+21, 2.7646178035599547e+23, 7.252342154467687e+25, 1.902486002142552e+28, 4.990736663077384e+30, 1.309205555895517e+33, 3.43440117822364e+35, 9.009365565146789e+37, 2.3634008863354504e+40, 6.199841386323181e+42, 1.6263865109725616e+45, 4.266452830404099e+47, 1.1192062668533923e+50, 2.935981522723795e+52, 7.701875656942412e+54, 2.0204108294240996e+57, 5.3000854616169215e+59, 1.3903561340764606e+62, 3.6472811496407546e+64, 9.56780745485834e+66, 2.509895336756791e+69, 6.584136053317718e+71, 1.727197422686752e+74, 4.530907187788945e+76, 1.1885798157586028e+79, 3.1179671528830246e+81, 8.179273312202984e+83, 2.1456451795477213e+86, 5.628609120626634e+88, 1.4765367981056114e+91, 3.873356400199367e+93, 1.0160864139799319e+96, 2.66547018658406e+98, 6.992251070201579e+100, 1.8342570580912127e+103, 4.811753641821725e+105, 1.2622534561038229e+108, 3.311233088905253e+110, 8.686262268517954e+112, 2.278642130337737e+115, 5.97749618609661e+117, 1.568059335824868e+120, 4.1134448339534026e+122, 1.0790681204086609e+125, 2.830688280710927e+127, 7.425662931752265e+129, 1.947952741802822e+132, 5.110008249999735e+134, 1.3404937273220829e+137, 3.5164785359983405e+139, 9.224676730744536e+141, 2.4198828434646216e+144, 6.348008875560657e+146, 1.6652548611197272e+149, 4.3684150524095865e+151, 1.1459537225002961e+154, 3.0061473517447214e+156, 7.885939652680404e+158, 2.0686958065985735e+161, 5.426750049734318e+163, 1.4235836901856333e+166, 3.7344460393228595e+168, 9.796464596187985e+170, 2.569878305210865e+173, 6.741487644597143e+175, 1.7684750118362855e+178, 4.6391894969886e+180, 1.2169852016524698e+183, 3.192482182507281e+185, 8.374746440455833e+187, 2.1969230815516916e+190, 5.763124962135427e+192, 1.5118239508744943e+195, 3.9659241704015926e+197, 1.0403694501782157e+200, 2.729171175137526e+202, 7.159356036382684e+204, 1.8780932219506644e+207, 4.926747786270438e+209, 1.2924195383821202e+212, 3.390366902577552e+214, 8.893851719761591e+216, 2.3330984724092664e+219, 6.1203499377706855e+221, 1.6055337485214682e+224, 4.2117503800449844e+226, 1.1048563308086631e+229, 2.8983377493396057e+231, 7.603125831843316e+233, 1.9945060725933244e+236, 5.23213026009751e+238, 1.372529642039841e+241, 3.60051742718444e+243] \n",
      " The weights after 100 iterations:\n",
      " [1.44697901e+118 1.44697901e+118 1.44697901e+118 1.44697901e+118\n",
      " 1.44697901e+118 1.44697901e+118 1.44697901e+118 1.44697901e+118\n",
      " 1.44697901e+118 1.44697901e+118]\n",
      "\n",
      "The loss with alpha 1e-07:\n",
      " [524.5718913821138, 501.9157970055969, 490.18224285903807, 484.10545439663974, 480.95829561120325, 479.3283872340995, 478.48426035685196, 478.04708843056363, 477.82067780641944, 477.70342011523906, 477.6426925491503, 477.6112418421992, 477.5949535726138, 477.5865179044902, 477.5821490857943, 477.5798864817518, 477.5787146828262, 477.5781078100832, 477.5777935116842, 477.5776307370584, 477.57754643634655, 477.57750277714604, 477.5774801661176, 477.5774684559031, 477.5774623912022, 477.57745925030343, 477.5774576236372, 477.57745678118937, 477.5774563448871, 477.57745611892676, 477.5774560019023, 477.57745594129557, 477.5774559099074, 477.5774558936516, 477.5774558852327, 477.5774558808726, 477.5774558786145, 477.577455877445, 477.57745587683934, 477.5774558765257, 477.57745587636316, 477.57745587627903, 477.5774558762355, 477.57745587621287, 477.57745587620127, 477.5774558761952, 477.57745587619206, 477.5774558761904, 477.57745587618956, 477.57745587618916, 477.5774558761889, 477.5774558761888, 477.57745587618876, 477.5774558761887, 477.57745587618876, 477.57745587618865, 477.5774558761887, 477.57745587618865, 477.57745587618865, 477.57745587618865, 477.57745587618865, 477.57745587618865, 477.5774558761887, 477.5774558761887, 477.5774558761887, 477.57745587618865, 477.57745587618865, 477.57745587618865, 477.5774558761887, 477.57745587618865, 477.57745587618865, 477.5774558761887, 477.5774558761887, 477.5774558761887, 477.57745587618876, 477.57745587618865, 477.5774558761887, 477.57745587618865, 477.57745587618865, 477.5774558761887, 477.57745587618865, 477.57745587618865, 477.57745587618876, 477.57745587618865, 477.57745587618865, 477.57745587618865, 477.5774558761887, 477.5774558761887, 477.5774558761887, 477.57745587618865, 477.5774558761887, 477.57745587618865, 477.57745587618865, 477.57745587618865, 477.57745587618865, 477.57745587618865, 477.57745587618865, 477.5774558761887, 477.5774558761887, 477.57745587618865, 477.57745587618865] \n",
      " The weights after 100 iterations:\n",
      " [0.05834689 0.04834689 0.02834689 0.00834689 0.01834689 0.01834689\n",
      " 0.03834689 0.02834689 0.01834689 0.00834689]\n",
      "\n",
      "The loss with alpha 1e-08:\n",
      " [524.5718913821138, 509.79880069854306, 499.6697536340831, 492.72485654845536, 487.96314547899146, 484.6983177890349, 482.45981576956916, 480.9250054313187, 479.872675456221, 479.15115413929294, 478.6564490343346, 478.31725858336654, 478.08469546470303, 477.9252404918164, 477.815911512551, 477.74095100494907, 477.6895549548105, 477.6543156794325, 477.630154162627, 477.6135880190816, 477.60222958019835, 477.5944417608116, 477.58910210705955, 477.5854410177641, 477.58293082226237, 477.581209727542, 477.58002967322585, 477.5792205787741, 477.5786658298758, 477.57828547089605, 477.57802468091717, 477.57784587243185, 477.5777232738772, 477.57763921519535, 477.5775815810576, 477.5775420646891, 477.57751497061895, 477.5774963937946, 477.57748365674905, 477.5774749236994, 477.5774689359564, 477.57746483051005, 477.57746201564487, 477.5774600856559, 477.577458762375, 477.5774578550783, 477.5774572329979, 477.5774568064737, 477.5774565140309, 477.57745631351986, 477.5774561760412, 477.57745608178016, 477.5774560171508, 477.5774559728381, 477.5774559424556, 477.577455921624, 477.577455907341, 477.5774558975481, 477.57745589083356, 477.5774558862298, 477.5774558830733, 477.5774558809091, 477.5774558794252, 477.5774558784078, 477.5774558777102, 477.5774558772319, 477.5774558769039, 477.57745587667904, 477.57745587652494, 477.5774558764192, 477.5774558763468, 477.57745587629705, 477.577455876263, 477.57745587623964, 477.5774558762236, 477.57745587621264, 477.57745587620514, 477.57745587619996, 477.57745587619644, 477.57745587619394, 477.5774558761923, 477.57745587619115, 477.5774558761904, 477.57745587618984, 477.57745587618956, 477.5774558761892, 477.5774558761891, 477.577455876189, 477.5774558761889, 477.57745587618876, 477.57745587618876, 477.57745587618876, 477.5774558761887, 477.5774558761887, 477.5774558761887, 477.57745587618865, 477.5774558761887, 477.5774558761887, 477.57745587618865, 477.5774558761887, 477.57745587618865] \n",
      " The weights after 100 iterations:\n",
      " [0.05834689 0.04834689 0.02834689 0.00834689 0.01834689 0.01834689\n",
      " 0.03834689 0.02834689 0.01834689 0.00834689]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 0.0000001 has the smallest loss after 100 iterations\n",
    "\n",
    "alpha_list = [1, 0.1, 0.01, 0.001, 0.0001, 0.00001, 0.000001, 0.0000001, 0.00000001]\n",
    "\n",
    "for alpha in alpha_list:\n",
    "    print(f'The loss with alpha {alpha}:\\n {gradient_descent(X,y.values,alpha,w,max_iterations)[0]} \\n The weights after {max_iterations} iterations:\\n {gradient_descent(X,y.values,alpha,w,max_iterations)[1]}', end='\\n'*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "11938c6bc6919ae2720b4d5011047913343b08a43b18698fd82dedb0d4417594"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
