{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\"The future king is the prince\",\n",
    "\"Daughter is the princess\",\n",
    "\"Son is the prince\", \n",
    "\"Only a man can be a king\", \n",
    "\"Only a woman can be a queen\", \n",
    "\"The princess will be a queen\",\n",
    "\"Queen and king rule the realm\",\n",
    "\"The prince is a strong man\",\n",
    "\"The princess is a beautiful woman\", \n",
    "\"The royal family is the king and queen and their children\",\n",
    "\"Prince is only a boy now\",\n",
    "\"A boy will be a man\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(\n",
    "    string: str, \n",
    "    punctuations=r'''!()-[]{};:'\"\\,<>./?@#$%^&*_~''',\n",
    "    stop_words=['the', 'a', 'and', 'is', 'be', 'will']) -> str:\n",
    "    \"\"\"\n",
    "    A method to clean text \n",
    "    \"\"\"\n",
    "    # Cleaning the urls\n",
    "    string = re.sub(r'https?://\\S+|www\\.\\S+', '', string)\n",
    "\n",
    "    # Cleaning the html elements\n",
    "    string = re.sub(r'<.*?>', '', string)\n",
    "\n",
    "    # Removing the punctuations\n",
    "    for x in string.lower(): \n",
    "        if x in punctuations: \n",
    "            string = string.replace(x, \"\") \n",
    "\n",
    "    # Converting the text to lower\n",
    "    string = string.lower()\n",
    "\n",
    "    # Removing stop words\n",
    "    string = ' '.join([word for word in string.split() if word not in stop_words])\n",
    "\n",
    "    # Cleaning the whitespaces\n",
    "    string = re.sub(r'\\s+', ' ', string).strip()\n",
    "\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the window for context\n",
    "window = 2\n",
    "\n",
    "# Creating a placeholder for the scanning of the word list\n",
    "word_lists = []\n",
    "all_text = []\n",
    "\n",
    "for text in texts:\n",
    "\n",
    "    # Cleaning the text\n",
    "    text = clean_text(text)\n",
    "\n",
    "    # Appending to the all text list\n",
    "    all_text += text.split() \n",
    "\n",
    "    # Creating a context dictionary\n",
    "    for i, word in enumerate(text.split()):\n",
    "        for w in range(window):\n",
    "            # Getting the context that is ahead by *window* words\n",
    "            if i + 1 + w < len(text.split()): \n",
    "                word_lists.append([word] + [text.split()[(i + 1 + w)]])\n",
    "            # Getting the context that is behind by *window* words    \n",
    "            if i - w - 1 >= 0:\n",
    "                word_lists.append([word] + [text.split()[(i - w - 1)]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['future', 'king'],\n",
       " ['future', 'prince'],\n",
       " ['king', 'prince'],\n",
       " ['king', 'future'],\n",
       " ['prince', 'king'],\n",
       " ['prince', 'future'],\n",
       " ['daughter', 'princess'],\n",
       " ['princess', 'daughter'],\n",
       " ['son', 'prince'],\n",
       " ['prince', 'son'],\n",
       " ['only', 'man'],\n",
       " ['only', 'can'],\n",
       " ['man', 'can'],\n",
       " ['man', 'only'],\n",
       " ['man', 'king'],\n",
       " ['can', 'king'],\n",
       " ['can', 'man'],\n",
       " ['can', 'only'],\n",
       " ['king', 'can'],\n",
       " ['king', 'man'],\n",
       " ['only', 'woman'],\n",
       " ['only', 'can'],\n",
       " ['woman', 'can'],\n",
       " ['woman', 'only'],\n",
       " ['woman', 'queen'],\n",
       " ['can', 'queen'],\n",
       " ['can', 'woman'],\n",
       " ['can', 'only'],\n",
       " ['queen', 'can'],\n",
       " ['queen', 'woman'],\n",
       " ['princess', 'queen'],\n",
       " ['queen', 'princess'],\n",
       " ['queen', 'king'],\n",
       " ['queen', 'rule'],\n",
       " ['king', 'rule'],\n",
       " ['king', 'queen'],\n",
       " ['king', 'realm'],\n",
       " ['rule', 'realm'],\n",
       " ['rule', 'king'],\n",
       " ['rule', 'queen'],\n",
       " ['realm', 'rule'],\n",
       " ['realm', 'king'],\n",
       " ['prince', 'strong'],\n",
       " ['prince', 'man'],\n",
       " ['strong', 'man'],\n",
       " ['strong', 'prince'],\n",
       " ['man', 'strong'],\n",
       " ['man', 'prince'],\n",
       " ['princess', 'beautiful'],\n",
       " ['princess', 'woman'],\n",
       " ['beautiful', 'woman'],\n",
       " ['beautiful', 'princess'],\n",
       " ['woman', 'beautiful'],\n",
       " ['woman', 'princess'],\n",
       " ['royal', 'family'],\n",
       " ['royal', 'king'],\n",
       " ['family', 'king'],\n",
       " ['family', 'royal'],\n",
       " ['family', 'queen'],\n",
       " ['king', 'queen'],\n",
       " ['king', 'family'],\n",
       " ['king', 'their'],\n",
       " ['king', 'royal'],\n",
       " ['queen', 'their'],\n",
       " ['queen', 'king'],\n",
       " ['queen', 'children'],\n",
       " ['queen', 'family'],\n",
       " ['their', 'children'],\n",
       " ['their', 'queen'],\n",
       " ['their', 'king'],\n",
       " ['children', 'their'],\n",
       " ['children', 'queen'],\n",
       " ['prince', 'only'],\n",
       " ['prince', 'boy'],\n",
       " ['only', 'boy'],\n",
       " ['only', 'prince'],\n",
       " ['only', 'now'],\n",
       " ['boy', 'now'],\n",
       " ['boy', 'only'],\n",
       " ['boy', 'prince'],\n",
       " ['now', 'boy'],\n",
       " ['now', 'only'],\n",
       " ['boy', 'man'],\n",
       " ['man', 'boy']]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['future',\n",
       " 'king',\n",
       " 'prince',\n",
       " 'daughter',\n",
       " 'princess',\n",
       " 'son',\n",
       " 'prince',\n",
       " 'only',\n",
       " 'man',\n",
       " 'can',\n",
       " 'king',\n",
       " 'only',\n",
       " 'woman',\n",
       " 'can',\n",
       " 'queen',\n",
       " 'princess',\n",
       " 'queen',\n",
       " 'queen',\n",
       " 'king',\n",
       " 'rule',\n",
       " 'realm',\n",
       " 'prince',\n",
       " 'strong',\n",
       " 'man',\n",
       " 'princess',\n",
       " 'beautiful',\n",
       " 'woman',\n",
       " 'royal',\n",
       " 'family',\n",
       " 'king',\n",
       " 'queen',\n",
       " 'their',\n",
       " 'children',\n",
       " 'prince',\n",
       " 'only',\n",
       " 'boy',\n",
       " 'now',\n",
       " 'boy',\n",
       " 'man']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_unique_word_dict(text:list) -> dict:\n",
    "    \"\"\"\n",
    "    A method that creates a dictionary where the keys are unique words\n",
    "    and key values are indices\n",
    "    \"\"\"\n",
    "    # Getting all the unique words from our text and sorting them alphabetically\n",
    "    words = list(set(text))\n",
    "    words.sort()\n",
    "\n",
    "    # Creating the dictionary for the unique words\n",
    "    unique_word_dict = {}\n",
    "    for i, word in enumerate(words):\n",
    "        unique_word_dict.update({\n",
    "            word: i\n",
    "        })\n",
    "\n",
    "    return unique_word_dict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'split'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [45], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m words \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mset\u001b[39m(texts\u001b[39m.\u001b[39;49msplit()))\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'split'"
     ]
    }
   ],
   "source": [
    "words = list(set(texts.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The prince is a strong man',\n",
       " 'Queen and king rule the realm',\n",
       " 'Only a woman can be a queen',\n",
       " 'The future king is the prince',\n",
       " 'A boy will be a man',\n",
       " 'Only a man can be a king',\n",
       " 'Prince is only a boy now',\n",
       " 'The princess is a beautiful woman',\n",
       " 'The princess will be a queen',\n",
       " 'Son is the prince',\n",
       " 'Daughter is the princess',\n",
       " 'The royal family is the king and queen and their children']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [40], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m create_unique_word_dict(word_lists)\n",
      "Cell \u001b[1;32mIn [31], line 7\u001b[0m, in \u001b[0;36mcreate_unique_word_dict\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39mA method that creates a dictionary where the keys are unique words\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[39mand key values are indices\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[39m# Getting all the unique words from our text and sorting them alphabetically\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m words \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mset\u001b[39;49m(text))\n\u001b[0;32m      8\u001b[0m words\u001b[39m.\u001b[39msort()\n\u001b[0;32m     10\u001b[0m \u001b[39m# Creating the dictionary for the unique words\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: unhashable type: 'list'"
     ]
    }
   ],
   "source": [
    "create_unique_word_dict(word_lists)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec in Python with Gensim Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\user\\appdata\\roaming\\python\\python39\\site-packages (4.11.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\user\\appdata\\roaming\\python\\python39\\site-packages (from beautifulsoup4) (2.3.2.post1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3 -> 22.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "! pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: lxml in c:\\users\\user\\appdata\\roaming\\python\\python39\\site-packages (4.9.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3 -> 22.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "! pip install lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4 as bs\n",
    "import urllib.request\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "scraped_data = urllib.request.urlopen('https://en.wikipedia.org/wiki/Artificial_intelligence')\n",
    "article = scraped_data.read()\n",
    "\n",
    "parsed_article = bs.BeautifulSoup(article,'lxml')\n",
    "\n",
    "paragraphs = parsed_article.find_all('p')\n",
    "\n",
    "article_text = \"\"\n",
    "\n",
    "for p in paragraphs:\n",
    "    article_text += p.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning the text\n",
    "processed_article = article_text.lower()\n",
    "processed_article = re.sub('[^a-zA-Z]', ' ', processed_article )\n",
    "processed_article = re.sub(r'\\s+', ' ', processed_article)\n",
    "\n",
    "# Preparing the dataset\n",
    "all_sentences = nltk.sent_tokenize(processed_article)\n",
    "\n",
    "all_words = [nltk.word_tokenize(sent) for sent in all_sentences]\n",
    "\n",
    "# Removing Stop Words\n",
    "from nltk.corpus import stopwords\n",
    "for i in range(len(all_words)):\n",
    "    all_words[i] = [w for w in all_words[i] if w not in stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.word2vec.Word2Vec at 0x1eceb222310>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "word2vec = Word2Vec(all_words, min_count=2)\n",
    "word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ai': 0, 'intelligence': 1, 'artificial': 2, 'learning': 3, 'human': 4, 'used': 5, 'research': 6, 'machine': 7, 'use': 8, 'many': 9, 'problems': 10, 'networks': 11, 'also': 12, 'intelligent': 13, 'data': 14, 'systems': 15, 'agent': 16, 'knowledge': 17, 'search': 18, 'world': 19, 'researchers': 20, 'field': 21, 'information': 22, 'neural': 23, 'algorithms': 24, 'computer': 25, 'general': 26, 'logic': 27, 'may': 28, 'symbolic': 29, 'machines': 30, 'would': 31, 'states': 32, 'system': 33, 'mind': 34, 'problem': 35, 'cyber': 36, 'example': 37, 'goal': 38, 'reasoning': 39, 'recognition': 40, 'one': 41, 'using': 42, 'could': 43, 'applications': 44, 'solve': 45, 'risk': 46, 'humans': 47, 'approaches': 48, 'diplomacy': 49, 'goals': 50, 'however': 51, 'technology': 52, 'decision': 53, 'specific': 54, 'computing': 55, 'theory': 56, 'since': 57, 'scientific': 58, 'developed': 59, 'optimization': 60, 'cybersecurity': 61, 'ability': 62, 'language': 63, 'program': 64, 'security': 65, 'mathematical': 66, 'russia': 67, 'two': 68, 'become': 69, 'number': 70, 'deep': 71, 'known': 72, 'including': 73, 'united': 74, 'tools': 75, 'include': 76, 'us': 77, 'algorithm': 78, 'well': 79, 'called': 80, 'even': 81, 'based': 82, 'often': 83, 'national': 84, 'software': 85, 'term': 86, 'behavior': 87, 'several': 88, 'uses': 89, 'science': 90, 'difficult': 91, 'future': 92, 'programs': 93, 'tasks': 94, 'neurons': 95, 'sub': 96, 'widely': 97, 'make': 98, 'perception': 99, 'first': 100, 'new': 101, 'level': 102, 'development': 103, 'definition': 104, 'form': 105, 'creating': 106, 'u': 107, 'google': 108, 'political': 109, 'fiction': 110, 'learn': 111, 'others': 112, 'planning': 113, 'objects': 114, 'processing': 115, 'nations': 116, 'process': 117, 'large': 118, 'particular': 119, 'robotics': 120, 'fields': 121, 'model': 122, 'know': 123, 'making': 124, 'agents': 125, 'input': 126, 'whether': 127, 'examples': 128, 'methods': 129, 'described': 130, 'solving': 131, 'techniques': 132, 'via': 133, 'time': 134, 'robots': 135, 'speech': 136, 'people': 137, 'foreign': 138, 'like': 139, 'beings': 140, 'successful': 141, 'market': 142, 'inputs': 143, 'translation': 144, 'application': 145, 'analysis': 146, 'actions': 147, 'question': 148, 'approach': 149, 'issue': 150, 'person': 151, 'require': 152, 'real': 153, 'formal': 154, 'president': 155, 'rather': 156, 'increase': 157, 'brain': 158, 'related': 159, 'inspired': 160, 'companies': 161, 'requires': 162, 'processes': 163, 'economics': 164, 'philosophy': 165, 'patent': 166, 'able': 167, 'patents': 168, 'considered': 169, 'similar': 170, 'vision': 171, 'early': 172, 'outputs': 173, 'easy': 174, 'network': 175, 'important': 176, 'choices': 177, 'china': 178, 'means': 179, 'turing': 180, 'classifiers': 181, 'simple': 182, 'together': 183, 'became': 184, 'environment': 185, 'due': 186, 'argues': 187, 'facts': 188, 'life': 189, 'relations': 190, 'solutions': 191, 'logics': 192, 'makes': 193, 'drones': 194, 'tool': 195, 'communication': 196, 'natural': 197, 'representation': 198, 'weapons': 199, 'ukraine': 200, 'cyberspace': 201, 'various': 202, 'industry': 203, 'statistical': 204, 'commonsense': 205, 'long': 206, 'way': 207, 'thus': 208, 'military': 209, 'according': 210, 'facebook': 211, 'change': 212, 'russell': 213, 'facial': 214, 'part': 215, 'institutions': 216, 'events': 217, 'argue': 218, 'multi': 219, 'students': 220, 'largest': 221, 'internet': 222, 'space': 223, 'much': 224, 'far': 225, 'modern': 226, 'popular': 227, 'allow': 228, 'prevent': 229, 'layers': 230, 'step': 231, 'identify': 232, 'classification': 233, 'soft': 234, 'experience': 235, 'billion': 236, 'help': 237, 'improve': 238, 'analyze': 239, 'paradigm': 240, 'calculus': 241, 'autonomous': 242, 'generation': 243, 'need': 244, 'target': 245, 'function': 246, 'order': 247, 'projects': 248, 'individual': 249, 'terms': 250, 'different': 251, 'go': 252, 'three': 253, 'election': 254, 'among': 255, 'bots': 256, 'works': 257, 'years': 258, 'surveillance': 259, 'advanced': 260, 'understanding': 261, 'things': 262, 'began': 263, 'effect': 264, 'ethics': 265, 'capable': 266, 'rights': 267, 'r': 268, 'leader': 269, 'digital': 270, 'common': 271, 'still': 272, 'jobs': 273, 'perform': 274, 'laws': 275, 'g': 276, 'potential': 277, 'bias': 278, 'highly': 279, 'german': 280, 'simulate': 281, 'technique': 282, 'principles': 283, 'ethical': 284, 'work': 285, 'member': 286, 'around': 287, 'computers': 288, 'misinformation': 289, 'previous': 290, 'feel': 291, 'class': 292, 'domains': 293, 'categories': 294, 'campaign': 295, 'central': 296, 'neuron': 297, 'set': 298, 'putin': 299, 'questions': 300, 'academic': 301, 'training': 302, 'followed': 303, 'funding': 304, 'success': 305, 'global': 306, 'k': 307, 'eu': 308, 'layer': 309, 'policy': 310, 'year': 311, 'image': 312, 'self': 313, 'forms': 314, 'diplomatic': 315, 'conflict': 316, 'hard': 317, 'viewed': 318, 'possible': 319, 'spam': 320, 'classifier': 321, 'reach': 322, 'strategy': 323, 'lead': 324, 'spread': 325, 'computational': 326, 'study': 327, 'sufficiently': 328, 'definitions': 329, 'company': 330, 'languages': 331, 'visual': 332, 'body': 333, 'classify': 334, 'partners': 335, 'youtube': 336, 'main': 337, 'designed': 338, 'e': 339, 'ministry': 340, 'functions': 341, 'truth': 342, 'affairs': 343, 'efforts': 344, 'certain': 345, 'top': 346, 'size': 347, 'guess': 348, 'swarm': 349, 'skills': 350, 'european': 351, 'patterns': 352, 'heuristics': 353, 'solution': 354, 'sector': 355, 'complex': 356, 'high': 357, 'established': 358, 'medical': 359, 'diagnosis': 360, 'introduced': 361, 'lethal': 362, 'test': 363, 'warfare': 364, 'show': 365, 'care': 366, 'logical': 367, 'english': 368, 'power': 369, 'especially': 370, 'directly': 371, 'decisions': 372, 'battlefield': 373, 'social': 374, 'emerging': 375, 'devices': 376, 'right': 377, 'made': 378, 'technologies': 379, 'japan': 380, 'consequences': 381, 'published': 382, 'public': 383, 'issues': 384, 'percent': 385, 'simulated': 386, 'countries': 387, 'leaders': 388, 'antiquity': 389, 'century': 390, 'evolutionary': 391, 'face': 392, 'move': 393, 'c': 394, 'included': 395, 'stated': 396, 'physical': 397, 'microsoft': 398, 'narrow': 399, 'although': 400, 'reason': 401, 'achieve': 402, 'assist': 403, 'assess': 404, 'fundamental': 405, 'task': 406, 'within': 407, 'philosophers': 408, 'movement': 409, 'failed': 410, 'ongoing': 411, 'defines': 412, 'object': 413, 'demonstrated': 414, 'governments': 415, 'given': 416, 'humanity': 417, 'competition': 418, 'dynamic': 419, 'major': 420, 'robot': 421, 'led': 422, 'design': 423, 'cognitive': 424, 'simon': 425, 'allows': 426, 'generally': 427, 'connectionist': 428, 'sources': 429, 'consider': 430, 'classifies': 431, 'minsky': 432, 'text': 433, 'working': 434, 'recent': 435, 'commercial': 436, 'computation': 437, 'procedural': 438, 'strategies': 439, 'word': 440, 'relationship': 441, 'marvin': 442, 'maximize': 443, 'searching': 444, 'steps': 445, 'middle': 446, 'heavily': 447, 'something': 448, 'regression': 449, 'rarely': 450, 'available': 451, 'winter': 452, 'focused': 453, 'ontology': 454, 'engineering': 455, 'fast': 456, 'concepts': 457, 'interest': 458, 'routine': 459, 'founded': 460, 'attention': 461, 'fuzzy': 462, 'concern': 463, 'involves': 464, 'current': 465, 'create': 466, 'web': 467, 'total': 468, 'amount': 469, 'reported': 470, 'rise': 471, 'increased': 472, 'late': 473, 'finding': 474, 'focus': 475, 'produce': 476, 'benchmarks': 477, 'accuracy': 478, 'animal': 479, 'algorithmic': 480, 'integrated': 481, 'properties': 482, 'ontologies': 483, 'wide': 484, 'experts': 485, 'state': 486, 'recommendation': 487, 'plan': 488, 'st': 489, 'amazon': 490, 'areas': 491, 'discovery': 492, 'support': 493, 'express': 494, 'statements': 495, 'breadth': 496, 'true': 497, 'assume': 498, 'default': 499, 'probability': 500, 'driving': 501, 'longer': 502, 'statistics': 503, 'typically': 504, 'knows': 505, 'automated': 506, 'game': 507, 'pattern': 508, 'chess': 509, 'area': 510, 'domain': 511, 'precisely': 512, 'straightforward': 513, 'defendants': 514, 'superintelligent': 515, 'creation': 516, 'bayesian': 517, 'performance': 518, 'government': 519, 'automation': 520, 'oxford': 521, 'role': 522, 'benefit': 523, 'playing': 524, 'back': 525, 'position': 526, 'spreading': 527, 'path': 528, 'adopted': 529, 'technological': 530, 'point': 531, 'defined': 532, 'extremely': 533, 'n': 534, 'match': 535, 'threat': 536, 'smart': 537, 'becomes': 538, 'handle': 539, 'programming': 540, 'international': 541, 'filed': 542, 'genetic': 543, 'friendly': 544, 'shift': 545, 'third': 546, 'identified': 547, 'trained': 548, 'begin': 549, 'likely': 550, 'asimov': 551, 'services': 552, 'chinese': 553, 'heart': 554, 'federal': 555, 'might': 556, 'better': 557, 'innovation': 558, 'open': 559, 'subjective': 560, 'tend': 561, 'sense': 562, 'private': 563, 'prominent': 564, 'relevant': 565, 'firms': 566, 'provides': 567, 'gradient': 568, 'attempts': 569, 'higher': 570, 'ibm': 571, 'created': 572, 'harm': 573, 'matching': 574, 'interaction': 575, 'champion': 576, 'business': 577, 'presented': 578, 'critics': 579, 'virtual': 580, 'philosopher': 581, 'proposed': 582, 'morality': 583, 'believe': 584, 'organizations': 585, 'founder': 586, 'simulation': 587, 'symbol': 588, 'measure': 589, 'games': 590, 'idea': 591, 'conscious': 592, 'moravec': 593, 'activation': 594, 'superintelligence': 595, 'risks': 596, 'becoming': 597, 'norvig': 598, 'said': 599, 'teaching': 600, 'considers': 601, 'race': 602, 'without': 603, 'addition': 604, 'operations': 605, 'deepfake': 606, 'increasing': 607, 'unlike': 608, 'white': 609, 'maintaining': 610, 'collar': 611, 'revolution': 612, 'centered': 613, 'reflect': 614, 'shut': 615, 'resources': 616, 'hawking': 617, 'stephen': 618, 'control': 619, 'korea': 620, 'south': 621, 'accountability': 622, 'manipulate': 623, 'upon': 624, 'claims': 625, 'compas': 626, 'unfair': 627, 'researching': 628, 'healthcare': 629, 'ranging': 630, 'range': 631, 'taking': 632, 'worth': 633, 'economists': 634, 'centuries': 635, 'name': 636, 'consciousness': 637, 'discussed': 638, 'explain': 639, 'explaining': 640, 'plans': 641, 'alan': 642, 'named': 643, 'peter': 644, 'stuart': 645, 'external': 646, 'act': 647, 'deduction': 648, 'internal': 649, 'along': 650, 'transhumanism': 651, 'cybernetics': 652, 'provably': 653, 'recognized': 654, 'neats': 655, 'complete': 656, 'impossible': 657, 'away': 658, 'emerged': 659, 'mistakes': 660, 'agree': 661, 'gofai': 662, 'necessary': 663, 'learned': 664, 'red': 665, 'apek': 666, 'karel': 667, 'assumption': 668, 'beyond': 669, 'raised': 670, 'philosophical': 671, 'arguments': 672, 'singularity': 673, 'possess': 674, 'institute': 675, 'explored': 676, 'hypothetical': 677, 'suffer': 678, 'scientists': 679, 'suggested': 680, 'strong': 681, 'existential': 682, 'searle': 683, 'rational': 684, 'beneficial': 685, 'appeared': 686, 'storytelling': 687, 'computationalism': 688, 'clear': 689, 'mary': 690, 'shelley': 691, 'frankenstein': 692, 'mankind': 693, 'developing': 694, 'share': 695, 'analyzing': 696, 'sway': 697, 'words': 698, 'wing': 699, 'perceives': 700, 'videos': 701, 'twitter': 702, 'component': 703, 'takes': 704, 'trump': 705, 'clinton': 706, 'miss': 707, 'coded': 708, 'achieving': 709, 'overwhelming': 710, 'engaging': 711, 'nation': 712, 'cset': 713, 'war': 714, 'gathers': 715, 'involving': 716, 'brings': 717, 'competence': 718, 'bring': 719, 'previously': 720, 'describe': 721, 'display': 722, 'associated': 723, 'service': 724, 'office': 725, 'governmental': 726, 'seen': 727, 'every': 728, 'establish': 729, 'deploying': 730, 'caused': 731, 'errors': 732, 'refugee': 733, 'un': 734, 'covid': 735, 'opposed': 736, 'displayed': 737, 'health': 738, 'gather': 739, 'society': 740, 'relay': 741, 'meta': 742, 'mitigate': 743, 'trustworthy': 744, 'normally': 745, 'biometric': 746, 'product': 747, 'medicine': 748, 'doctors': 749, 'scans': 750, 'diagnose': 751, 'employ': 752, 'enables': 753, 'handling': 754, 'economic': 755, 'vital': 756, 'riiid': 757, 'live': 758, 'identity': 759, 'authentication': 760, 'hub': 761, 'rejected': 762, 'helping': 763, 'simulating': 764, 'novel': 765, 'frequently': 766, 'almost': 767, 'series': 768, 'culture': 769, 'day': 770, 'rare': 771, 'experienced': 772, 'waves': 773, 'commission': 774, 'calling': 775, 'trust': 776, 'values': 777, 'russian': 778, 'canada': 779, 'phenomenon': 780, 'released': 781, 'regulation': 782, 'databases': 783, 'investment': 784, 'jurisdictions': 785, 'responsibility': 786, 'decades': 787, 'responsible': 788, 'tech': 789, 'serious': 790, 'dominated': 791, 'musk': 792, 'proved': 793, 'bill': 794, 'dick': 795, 'following': 796, 'behalf': 797, 'netflix': 798, 'effort': 799, 'mission': 800, 'acting': 801, 'limit': 802, 'september': 803, 'overall': 804, 'diplomats': 805, 'detected': 806, 'engines': 807, 'agenda': 808, 'authority': 809, 'czech': 810, 'unique': 811, 'defence': 812, 'line': 813, 'nuclear': 814, 'siri': 815, 'alexa': 816, 'malicious': 817, 'secure': 818, 'cars': 819, 'deployed': 820, 'competing': 821, 'highest': 822, 'take': 823, 'ukrainian': 824, 'come': 825, 'taken': 826, 'intentions': 827, 'increasingly': 828, 'proponents': 829, 'exactly': 830, 'newell': 831, 'told': 832, 'markov': 833, 'enormous': 834, 'prediction': 835, 'filtering': 836, 'retrieval': 837, 'devised': 838, 'operate': 839, 'belief': 840, 'situation': 841, 'degree': 842, 'predictions': 843, 'ant': 844, 'utility': 845, 'value': 846, 'allowing': 847, 'guesses': 848, 'landscape': 849, 'uncertainty': 850, 'random': 851, 'blind': 852, 'cooperation': 853, 'came': 854, 'smaller': 855, 'best': 856, 'ml': 857, 'supply': 858, 'concept': 859, 'tree': 860, 'serve': 861, 'changing': 862, 'models': 863, 'incomplete': 864, 'key': 865, 'explosion': 866, 'exponentially': 867, 'depends': 868, 'scalability': 869, 'intelligently': 870, 'bayes': 871, 'naive': 872, 'exists': 873, 'displaced': 874, 'svm': 875, 'neighbor': 876, 'nearest': 877, 'interpret': 878, 'attempt': 879, 'classified': 880, 'observation': 881, 'combined': 882, 'observations': 883, 'average': 884, 'therefore': 885, 'represented': 886, 'description': 887, 'represent': 888, 'pick': 889, 'controllers': 890, 'diamond': 891, 'divided': 892, 'simplest': 893, 'valuable': 894, 'finds': 895, 'never': 896, 'supervised': 897, 'result': 898, 'translators': 899, 'develop': 900, 'answering': 901, 'translate': 902, 'structure': 903, 'useful': 904, 'variety': 905, 'occurrence': 906, 'appears': 907, 'near': 908, 'another': 909, 'sentiment': 910, 'generate': 911, 'sensors': 912, 'signals': 913, 'active': 914, 'successes': 915, 'existing': 916, 'give': 917, 'otherwise': 918, 'location': 919, 'visible': 920, 'programmed': 921, 'interior': 922, 'assistants': 923, 'motion': 924, 'affective': 925, 'despite': 926, 'joint': 927, 'texts': 928, 'hans': 929, 'written': 930, 'proof': 931, 'grows': 932, 'quickly': 933, 'determine': 934, 'sufficient': 935, 'belongs': 936, 'searches': 937, 'local': 938, 'moving': 939, 'find': 940, 'attempting': 941, 'inference': 942, 'learners': 943, 'trying': 944, 'critical': 945, 'powerful': 946, 'maps': 947, 'good': 948, 'responses': 949, 'ones': 950, 'features': 951, 'applied': 952, 'hopes': 953, 'complexity': 954, 'sample': 955, 'required': 956, 'nlp': 957, 'architecture': 958, 'understand': 959, 'distribution': 960, 'uncertain': 961, 'herbert': 962, 'defense': 963, 'eventually': 964, 'predicted': 965, 'university': 966, 'twenty': 967, 'agreed': 968, 'writing': 969, 'substantially': 970, 'machinery': 971, 'recognize': 972, 'difficulty': 973, 'remaining': 974, 'response': 975, 'wrote': 976, 'agriculture': 977, 'british': 978, 'entertainment': 979, 'next': 980, 'later': 981, 'sectors': 982, 'personal': 983, 'refer': 984, 'revived': 985, 'inventions': 986, 'four': 987, 'papers': 988, 'fifth': 989, 'project': 990, 'hours': 991, 'beginning': 992, 'action': 993, 'department': 994, 'dealing': 995, 'funded': 996, 'legal': 997, 'see': 998, 'second': 999, 'history': 1000, 'sought': 1001, 'ways': 1002, 'guided': 1003, 'connections': 1004, 'james': 1005, 'manyika': 1006, 'period': 1007, 'directed': 1008, 'john': 1009, 'intellectual': 1010, 'pushed': 1011, 'gained': 1012, 'prominence': 1013, 'pigeons': 1014, 'movements': 1015, 'define': 1016, 'produced': 1017, 'press': 1018, 'matter': 1019, 'algebra': 1020, 'thing': 1021, 'theorems': 1022, 'broader': 1023, 'rigid': 1024, 'argued': 1025, 'lisp': 1026, 'diverse': 1027, 'generalized': 1028, 'deepmind': 1029, 'improved': 1030, 'multiple': 1031, 'perceptrons': 1032, 'short': 1033, 'usage': 1034, 'recurrent': 1035, 'signal': 1036, 'feedforward': 1037, 'infrastructure': 1038, 'survey': 1039, 'five': 1040, 'competitive': 1041, 'incorporated': 1042, 'descent': 1043, 'numerous': 1044, 'concerned': 1045, 'fully': 1046, 'spectrum': 1047, 'continuous': 1048, 'agi': 1049, 'wire': 1050, 'traits': 1051, 'capabilities': 1052, 'expect': 1053, 'received': 1054, 'fire': 1055, 'weighted': 1056, 'puzzles': 1057, 'deductions': 1058, 'started': 1059, 'convolutional': 1060, 'hungry': 1061, 'targeted': 1062, 'superhuman': 1063, 'rodney': 1064, 'brooks': 1065, 'jeopardy': 1066, 'beat': 1067, 'survive': 1068, 'blue': 1069, 'j': 1070, 'david': 1071, 'deepfakes': 1072, 'thousands': 1073, 'id': 1074, 'apple': 1075, 'daily': 1076, 'advances': 1077, 'mainstream': 1078, 'list': 1079, 'includes': 1080, 'hardware': 1081, 'verifiable': 1082, 'results': 1083, 'mathematics': 1084, 'specialized': 1085, 'cases': 1086, 'rnn': 1087, 'access': 1088, 'reduce': 1089, 'enabled': 1090, 'textbooks': 1091}\n"
     ]
    }
   ],
   "source": [
    "vocabulary = word2vec.wv.key_to_index\n",
    "print(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "v1 = word2vec.wv['artificial']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ai', 0.613765299320221),\n",
       " ('used', 0.535531222820282),\n",
       " ('artificial', 0.5345665216445923),\n",
       " ('search', 0.5322884321212769),\n",
       " ('information', 0.4942713975906372),\n",
       " ('logic', 0.4893563985824585),\n",
       " ('learning', 0.4892669916152954),\n",
       " ('networks', 0.48703572154045105),\n",
       " ('knowledge', 0.4859626889228821),\n",
       " ('use', 0.4855497479438782)]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_words = word2vec.wv.most_similar('intelligence')\n",
    "sim_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "11938c6bc6919ae2720b4d5011047913343b08a43b18698fd82dedb0d4417594"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
